{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of This Notebook  \n",
    "1) Want to experience tuning models' parameter & several strategy experiment  \n",
    "2) Experience Token Classification Task  \n",
    "3) Study Top Ranker's Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# !unzip /home/qcqced/nltk_data/corpora/wordnet.zip -d /home/qcqced/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.swa_utils as swa\n",
    "import tokenizers, transformers\n",
    "import os, sys, gc, time, random, warnings, math, re\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AdamW\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from numpy import ndarray\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
    "from tqdm.auto import tqdm\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "from glob import glob\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/qcqced/.netrc\n"
     ]
    }
   ],
   "source": [
    "# WandB Login => Copy API Key\n",
    "secret_value_0 = '8d7716caaaa5afb56e1d02ef5837cabbffe48b41'\n",
    "!wandb login $secret_value_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1 Configuration Setting\n",
    "\"\"\"\n",
    "[Configuration]\n",
    "    - Pooling: mean, attention, max, weightedlayer, concat (This Pipeline doesn't need to pooling)\n",
    "    - Optimizer: AdamW, SWA\n",
    "    - Scheduler: cosine, linear\n",
    "    - Clip_grad_norm, Gradient Checking: T/F\n",
    "    - LLRD\n",
    "    - Re-Init\n",
    "    - AWP\n",
    "\"\"\"\n",
    "class CFG:\n",
    "    \"\"\"--------[Common]--------\"\"\"\n",
    "    wandb, train, competition, seed, cfg_name = True, True, 'UPPPM', 42, 'CFG'\n",
    "    device, gpu_id = torch.device('cuda' if torch.cuda.is_available() else 'cpu'), 0\n",
    "    num_workers = 0\n",
    "    \"\"\" Mixed Precision, Gradient Check Point \"\"\"\n",
    "    amp_scaler = True\n",
    "    gradient_checkpoint = True # save parameter\n",
    "    output_dir = './output/'\n",
    "    \"\"\" Clipping Grad Norm, Gradient Accumulation \"\"\"\n",
    "    clipping_grad = True # clip_grad_norm\n",
    "    n_gradient_accumulation_steps = 1 # Gradient Accumulation\n",
    "    max_grad_norm = n_gradient_accumulation_steps * 1000\n",
    "    \"\"\" Model \"\"\"\n",
    "    model_name = 'microsoft/deberta-v3-large' \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#    pooling = 'attention'\n",
    "    max_len = 512\n",
    "    \"\"\" CV, Epoch, Batch Size \"\"\"\n",
    "    n_folds = 4\n",
    "    epochs = 180\n",
    "    batch_size = 64\n",
    "    \"\"\" SWA, Loss, Optimizer, Scheduler \"\"\"\n",
    "    swa = True\n",
    "    swa_start = int(epochs*0.75)\n",
    "    swa_lr = 1e-4\n",
    "    anneal_epochs = 4\n",
    "    anneal_strategy = 'cos' # default = cos, available option: linear \n",
    "    loss_fn = 'BCE'\n",
    "    optimizer = 'AdamW' # options: SWA, AdamW\n",
    "    weight_decay = 1e-2\n",
    "    scheduler = 'cosine_annealing' # options: cosine, linear, cosine_annealing, linearannealing\n",
    "    num_cycles = 0.5\n",
    "#    num_warmup_steps = 0\n",
    "    warmup_ratio = 0.1 # options: 0.05, 0.1\n",
    "    batch_scheduler = True\n",
    "    # encoder_lr = 5e-5\n",
    "    # decoder_lr = 1e-5\n",
    "    min_lr = 1e-7\n",
    "    # eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    \"\"\" LLRD \"\"\"\n",
    "    llrd = True\n",
    "    layerwise_lr = 5e-5\n",
    "    layerwise_lr_decay = 0.9\n",
    "    layerwise_weight_decay = 1e-2\n",
    "    layerwise_adam_epsilon = 1e-6\n",
    "    layerwise_use_bertadam = False\n",
    "    \"\"\" Re-Init, AWP \"\"\"\n",
    "    reinit = True\n",
    "    num_reinit = 5\n",
    "    awp = False\n",
    "    nth_awp_start_epoch = 10\n",
    "    awp_eps = 1e-2\n",
    "    awp_lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f245eefac70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_device() -> bool:\n",
    "    return torch.backends.mps.is_available()\n",
    "\n",
    "def check_library(checker: bool) -> tuple:\n",
    "    \"\"\"\n",
    "    1) checker == True \n",
    "        - current device is mps\n",
    "    2) checker == False\n",
    "        - current device is cuda with cudnn\n",
    "    \"\"\"\n",
    "    if not checker:\n",
    "        _is_built = torch.backends.cudnn.is_available()\n",
    "        _is_enable = torch.backends.cudnn.enabledtorch.backends.cudnn.enabled\n",
    "        version = torch.backends.cudnn.version()\n",
    "        device = (_is_built, _is_enable, version)\n",
    "        return device\n",
    "\n",
    "def class2dict(cfg: CFG) -> dict:\n",
    "    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))\n",
    "\n",
    "def all_type_seed(cfg: CFG, checker: bool) -> None:\n",
    "    # python & torch seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)  # python Seed\n",
    "    random.seed(cfg.seed)  # random module Seed\n",
    "    np.random.seed(cfg.seed)  # numpy module Seed\n",
    "    torch.manual_seed(cfg.seed)  # Pytorch CPU Random Seed Maker\n",
    "\n",
    "    # device == cuda\n",
    "    if not checker: \n",
    "        torch.cuda.manual_seed(cfg.seed)  # Pytorch GPU Random Seed Maker\n",
    "        torch.cuda.manual_seed_all(cfg.seed)  # Pytorch Multi Core GPU Random Seed Maker\n",
    "        # torch.cudnn seed\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # devide == mps\n",
    "#     else:\n",
    "#         torch.mps.manual_seed(cfg.seed)\n",
    "    \n",
    "def seed_worker(worker_id) -> None:\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "def get_logger(filename: str):\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "if not os.path.exists(CFG.output_dir):\n",
    "    os.makedirs(CFG.output_dir)\n",
    "\n",
    "logger = get_logger(filename=CFG.output_dir+'train')\n",
    "check_library(True)\n",
    "all_type_seed(CFG, True)    \n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 1. Stratified Group K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Stratified Group K]\n",
    "Í∞ôÏùÄ Í∑∏Î£πÏùÄ ÏÑúÎ°ú Í∞ôÏùÄ ÏÑ±ÏßàÏùò Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïóê Ìè¨Ìï®ÎêòÎ©∞, Ï†ÑÏ≤¥ ÎùºÎ≤® Î∂ÑÌè¨Î•º Í≥†Î†§Ìïú Ìè¥Îìú Íµ¨ÏÑ± Ï†ÑÎûµ\n",
    "1) Ï†êÏàòÎ•º ÌÅ¥ÎûòÏä§Î°ú ÏπòÌôò\n",
    "    - 0.00: class 0\n",
    "    - 0.25: class 1\n",
    "    - 0.50: class 2\n",
    "    - 0.75: class 3\n",
    "    - 1.00: class 4\n",
    "2) ÎùºÎ≤® Î∂ÑÌè¨ Í∑†ÌòïÏùÑ ÏúÑÌï¥ StratifiedGroupKFold ÏÇ¨Ïö©\n",
    "3) Group Í∏∞Ï§ÄÏùÄ 'anchor'\n",
    "4) ÏõêÏûëÏûêÎäî Ìè¥ÎìúÎ•º 2Í∞úÎ°ú Íµ¨ÏÑ±ÌïòÍ≥† ÌÖåÏä§Ìä∏Î•º ÎèåÎ¶∞ÎìØ\n",
    "\"\"\"\n",
    "def cross_val(cfg):\n",
    "    train_df = pd.read_csv('./dataset/train.csv')\n",
    "    kfold = StratifiedGroupKFold(\n",
    "        n_splits=CFG.n_folds, \n",
    "        shuffle=True, \n",
    "        random_state=cfg.seed\n",
    "    )\n",
    "    train_df[\"score_class\"] = train_df[\"score\"].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n",
    "    train_df['fold'] = -1\n",
    "    for num, (tx, vx) in enumerate(kfold.split(train_df, train_df[\"score_class\"], train_df[\"anchor\"])):\n",
    "        train_df.loc[vx, \"fold\"] = num\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Stage 2. Text Preprocessing: Normalization & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"[Text Normalization]\"\"\"\n",
    "def create_word_normalizer() -> function:\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def normalize(word):\n",
    "        w = word.lower()\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "        w = ps.stem(w)\n",
    "        return w\n",
    "    return normalize\n",
    "\n",
    "def __normalize_words(titles: list) -> list:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    normalizer = create_word_normalizer()\n",
    "    titles = [normalizer(t) for t in titles if t not in stop_words]\n",
    "    return titles\n",
    "\n",
    "def normalize_words(words: ndarray, unique=True):\n",
    "    if type(words) is str:\n",
    "        words = [words]\n",
    "    sep_re = r'[\\s\\(\\){}\\[\\];,\\.]+'\n",
    "    num_re = r'\\d'\n",
    "    words = re.split(sep_re, ' '.join(words).lower())\n",
    "    words = [w for w in words if len(w) >= 3 and not re.match(num_re, w)]\n",
    "    if unique:\n",
    "        words = list(set(words))\n",
    "        words = set(__normalize_words(words))\n",
    "    else:\n",
    "        words = __normalize_words(words)\n",
    "    return words\n",
    "\n",
    "def filter_title(title: str) -> str:\n",
    "    titles = normalize_words(title, unique=False)\n",
    "    return ','.join([t for t in titles if t in include_words])\n",
    "\n",
    "train_df = cross_val(CFG)\n",
    "cpc_codes = pd.read_csv(\"./dataset/titles.csv\", engine='python')\n",
    "\n",
    "norm_titles = normalize_words(cpc_codes['title'].to_numpy(), unique=False) # Ïó¨Í∏∞Îäî big query datasetÏùÑ Ï†ïÍ∑úÌôî\n",
    "anchor_targets = train_df['target'].unique().tolist() + train_df['anchor'].unique().tolist() # original train dataset \n",
    "norm_anchor_targets = normalize_words(anchor_targets) # Original Train Dataset Ï†ïÍ∑úÌôî\n",
    "include_words = set(norm_titles) & norm_anchor_targets # Anchor & Target Í≥µÌÜµÎêòÎäî Îã®Ïñ¥ \n",
    "\n",
    "# Ïôú ÏΩîÎìú Í∏∏Ïù¥Í∞Ä 4Í∞ú Ïù¥ÏÉÅÏù∏ Í≤ÉÎßå Í∞ÄÏ†∏Ïò§Îäî Í≤ÉÏù∏ÏßÄ... ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞Îäî sub-classÍπåÏßÄ Ï†úÍ≥µ ÏïàÌïòÎäî Í±∏Î°ú ÏïÑÎäîÎç∞\n",
    "tmp_cpc_codes = cpc_codes.copy()\n",
    "tmp_cpc_codes = tmp_cpc_codes[cpc_codes['code'].str.len() >= 4] \n",
    "\n",
    "\"\"\"\n",
    "1) ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞Ï≤òÎüº ÌòïÌÉúÎ•º ÎßûÏ∂∞Ï£ºÎ†§Í≥† 3ÏûêÎ¶¨ÍπåÏßÄ Ïä¨ÎùºÏù¥Ïã±Ìï¥ÏÑú ÏÉàÎ°úÏö¥ Ïª¨ÎüºÏùÑ ÎßåÎì†Îã§.\n",
    "2) section_class 1: ['sub_class 1', 'sub_class 2', 'sub_class 3', .....]\n",
    "    - sub_classÏùò sub_class textÍπåÏßÄ Î™®Îëê Ìè¨Ìï®\n",
    "3) Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò Í≤ÄÏ¶ù\n",
    "4) Î¶¨Ïä§Ìä∏ ÌòïÌÉú Î≠âÍ∞úÍ≥†, Ï§ÑÍ∏ÄÎ°ú Î≥ÄÌôò & Ï†ïÍ∑úÌôî\n",
    "5) Anchor & Target ÍµêÏßëÌï© Îç∞Ïù¥ÌÑ∞ Îî∞Î°ú Î™®ÏïÑ ÏÉàÎ°úÏö¥ Ïó¥Ïóê ÏÇΩÏûÖ\n",
    "    - Ïù¥ÌõÑ CountVectorizer Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌï®Ïù∏ÎìØ\n",
    "\"\"\"\n",
    "tmp_cpc_codes['section_class'] = tmp_cpc_codes['code'].apply(lambda x: x[:3]) \n",
    "title_group_df = tmp_cpc_codes.groupby('section_class', as_index=False)[['title']].agg(list)\n",
    "title_group_df = title_group_df[title_group_df['section_class'].str.len() == 3] \n",
    "title_group_df['title'] = title_group_df['title'].apply(lambda lst: ' '.join(lst))\n",
    "title_group_df['norm_title'] = title_group_df['title'].agg(filter_title)\n",
    "\n",
    "\"\"\"\n",
    "1) CountVectorizer Ï†ÅÏö©\n",
    " - Í∞úÎ≥Ñ Îã®Ïñ¥Ïùò Ï∂úÌòÑÎπàÎèÑ Í≥ÑÏÇ∞\n",
    "2) Î≤°ÌÑ∞Î•º Îã§Ïãú ÏõêÎ≥∏ Îã®Ïñ¥Î°ú Î≥ÄÌôò\n",
    "\"\"\"\n",
    "vectorizer = CountVectorizer()\n",
    "c_vect = vectorizer.fit_transform(title_group_df['norm_title'])\n",
    "r = np.argsort(c_vect.toarray(), axis=1)[:, ::-1][::, :400]\n",
    "vect_words = vectorizer.get_feature_names_out()\n",
    "t_words = np.vectorize(lambda v: vect_words[v])(r)\n",
    "norm_title = title_group_df['norm_title'].str.split(',').to_numpy().tolist()\n",
    "\n",
    "\"\"\"\n",
    "1) Ïù¥Ìï¥Í∞Ä ÏïàÍ∞ê\n",
    "2) class Ï†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "    - sub_class Ï†úÎ™© Ï∂îÍ∞Ä\n",
    "3) context_text: main class + [SEP] + sub class\n",
    "4) cpc_text => {'A01': ('A01's title [SEP] A01B's title.........)}\n",
    "\"\"\"\n",
    "res = []\n",
    "for (n, t) in zip(norm_title, t_words):\n",
    "    res.append(','.join(set(n) & set(t)))\n",
    "title_group_df['norm_title'] = res\n",
    "title_group_df['section'] = title_group_df.section_class.str[0:1]\n",
    "title_group_df['section_title'] = title_group_df['section'].map(cpc_codes.set_index('code')['title']).str.lower() + ';' + title_group_df['section_class'].map(cpc_codes.set_index('code')['title']).str.lower()\n",
    "title_group_df['context_text'] = title_group_df['section_title'] + ' [SEP] ' + title_group_df['norm_title']\n",
    "cpc_texts = dict(title_group_df[['section_class', 'context_text']].to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"[Merge Two DataFrame]\"\"\"\n",
    "af_dict = {}\n",
    "for i,r in train_df[['anchor', 'fold']].iterrows():\n",
    "    af_dict[r.anchor] = r.fold\n",
    "anchor_context_grouped_target = train_df.groupby(['anchor', 'context'])['target'].apply(list)\n",
    "anchor_context_grouped_score = train_df.groupby(['anchor', 'context'])['score'].apply(list)\n",
    "anchor_context_grouped_id = train_df.groupby(['anchor', 'context'])['id'].apply(list)\n",
    "i = pd.DataFrame(anchor_context_grouped_id).reset_index()\n",
    "s = pd.DataFrame(anchor_context_grouped_score).reset_index()\n",
    "t = pd.DataFrame(anchor_context_grouped_target).reset_index()\n",
    "train_df = s.merge(t, on=['anchor', 'context'])\n",
    "train_df = train_df.merge(i, on=['anchor', 'context'])\n",
    "train_df['context_text'] = train_df['context'].map(cpc_texts)\n",
    "train_df = train_df.rename(columns={'target': 'targets', 'score': 'scores', 'id': 'ids'})\n",
    "train_df['fold'] = train_df['anchor'].map(af_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./output/tokenizer/tokenizer_config.json',\n",
       " './output/tokenizer/special_tokens_map.json',\n",
       " './output/tokenizer/spm.model',\n",
       " './output/tokenizer/added_tokens.json',\n",
       " './output/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Special Token\n",
    "tar_token = '[TAR]'\n",
    "special_tokens_dict = {'additional_special_tokens': [f'{tar_token}']}\n",
    "CFG.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tar_token_id = CFG.tokenizer(f'{tar_token}', add_special_tokens=False)['input_ids'][0]\n",
    "# logger.info(f'tar_token_id: {tar_token_id}')\n",
    "setattr(CFG.tokenizer, 'tar_token', f'{tar_token}')\n",
    "setattr(CFG.tokenizer, 'tar_token_id', tar_token_id)\n",
    "CFG.tokenizer.save_pretrained(f'{CFG.output_dir}tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of Train Data (anchor + targets + context_text)\n",
    "# token_test = train_df.explode('targets')\n",
    "\n",
    "# anchor_list = token_test.anchor.to_list()\n",
    "# targets_list = token_test.targets.to_list()\n",
    "# context_list = token_test.context_text.to_list()\n",
    "\n",
    "# text_len, text_token = [], []\n",
    "# text_list = [anchor_list[idx] + targets_list[idx] + context_list[idx] for idx in range(len(anchor_list))]\n",
    "# for text in tqdm(text_list):\n",
    "#     inputs = CFG.tokenizer.encode_plus(\n",
    "#         text,\n",
    "#         return_tensors = None,\n",
    "#         add_special_tokens = False,\n",
    "#         truncation = False,\n",
    "# #        max_length = 192,\n",
    "#         return_token_type_ids = True,\n",
    "#         return_attention_mask = True    \n",
    "#     )\n",
    "#     text_token.append(CFG.tokenizer.decode(inputs.input_ids))\n",
    "#     text_len.append(len(inputs.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Step 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UPPPMDataset(Dataset):\n",
    "    def __init__(self, CFG, df, is_valid=False):\n",
    "        super().__init__()\n",
    "        self.anchor_list = df.anchor.to_numpy()\n",
    "        self.target_list = df.targets.to_numpy()\n",
    "        self.context_list = df.context_text.to_numpy()\n",
    "        self.score_list = df.scores.to_numpy()\n",
    "        self.id_list = df.ids.to_numpy()\n",
    "        self.cfg = CFG\n",
    "        self.is_valid = is_valid\n",
    "        \n",
    "#     def add_special_token(self):\n",
    "#         tar_token = '[TAR]'\n",
    "#         special_tokens_dict = {'additional_special_tokens': [f'{tar_token}']}\n",
    "#         self.cfg.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "#         tar_token_id = self.cfg.tokenizer(f'{tar_token}', add_special_tokens=False)['input_ids'][0]\n",
    "#         # logger.info(f'tar_token_id: {tar_token_id}')\n",
    "#         setattr(self.cfg.tokenizer, 'tar_token', f'{tar_token}')\n",
    "#         setattr(self.cfg.tokenizer, 'tar_token_id', tar_token_id)\n",
    "#         self.cfg.tokenizer.save_pretrained(f'{self.cfg.output_dir}tokenizer/')\n",
    "    \n",
    "    def tokenizing(self, text_data: str) -> dict:\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            text_data, \n",
    "            return_tensors=None, # if true, tf.tensor, pt.tensor, numpy\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.cfg.max_len\n",
    "        )\n",
    "        return inputs\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.id_list)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> \"tuple[dict, Tensor, Tensor]\":\n",
    "        \"\"\"\n",
    "        1) make Embedding Shape,\n",
    "            - Data: [cls]+[anchor]+[sep]+[target]+[tar]+[target]+[tar]...+[tar]+[cpc_text]+[sep]\n",
    "            - Label: [-1] * self.cfg.max_len, target valueÏùò Ïù∏Îç±Ïä§ ÏúÑÏπòÏóê score_classÍ∞í Ï†ÑÎã¨\n",
    "        2) apply data augment\n",
    "            - shuffle target values\n",
    "        \"\"\"\n",
    "        scores = np.array(self.score_list[idx]) # len(scores) == target count\n",
    "        target_mask = np.zeros(self.cfg.max_len)\n",
    "        targets = np.array(self.target_list[idx])\n",
    "        \n",
    "        # Data Augment for train stage\n",
    "        if not self.is_valid:\n",
    "            indices = list(range(len(scores)))\n",
    "            random.shuffle(indices)\n",
    "            scores = scores[indices]\n",
    "            targets = targets[indices]\n",
    "        \n",
    "        text = self.cfg.tokenizer.cls_token + self.anchor_list[idx] + self.cfg.tokenizer.sep_token\n",
    "        for target in targets:\n",
    "            text += target + self.cfg.tokenizer.tar_token\n",
    "        text += self.context_list[idx] + self.cfg.tokenizer.sep_token\n",
    "        \n",
    "        # tokenizing & make label list\n",
    "        inputs = self.tokenizing(text) \n",
    "        label = torch.full([self.cfg.max_len], -1, dtype=torch.float)\n",
    "        # target valueÏùò Ïù∏Îç±Ïä§(label list) ÏúÑÏπòÏóê scoreÍ∞í ÏûêÏ≤¥Î•º Ï†ÑÎã¨\n",
    "        # ÎÇòÏ§ëÏóê score_classÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî ÏûëÏóÖÏù¥ ÌïÑÏöîÌï† ÎìØ\n",
    "        cnt_tar = 0\n",
    "        cnt_sep = 0\n",
    "        nth_target = -1\n",
    "        prev_i = -1\n",
    "        \n",
    "        for i, input_id in enumerate(inputs['input_ids']):\n",
    "            if input_id == self.cfg.tokenizer.tar_token_id:\n",
    "                cnt_tar += 1\n",
    "                if cnt_tar == len(targets):\n",
    "                    break\n",
    "            if input_id == self.cfg.tokenizer.sep_token_id:\n",
    "                cnt_sep += 1\n",
    "            \n",
    "            if cnt_sep == 1 and input_id not in [self.cfg.tokenizer.pad_token_id, self.cfg.tokenizer.sep_token_id, self.cfg.tokenizer.tar_token_id]:\n",
    "                if (i-prev_i) > 1:\n",
    "                    nth_target += 1\n",
    "                label[i] = scores[nth_target]\n",
    "                target_mask[i] = 1\n",
    "                prev_i = i\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "            \n",
    "        return inputs, target_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 4. Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UPPPMModel(nn.Module):\n",
    "    def __init__(self, CFG, n_vocabs: int):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG\n",
    "        self.auto_cfg = AutoConfig.from_pretrained(CFG.model_name,\n",
    "                                                   output_hidden_states = True)\n",
    "        self.model = AutoModel.from_pretrained(CFG.model_name,\n",
    "                                               config = self.auto_cfg)\n",
    "        self.model.resize_token_embeddings(n_vocabs)\n",
    "        \"\"\"\n",
    "        1) if your loss function == CrossEntropyLoss, change value 1 to 5\n",
    "        2) fully_connected == classifier        \n",
    "        \"\"\"\n",
    "        self.fc = nn.Linear(self.auto_cfg.hidden_size, 1)\n",
    "        self._init_weights(self.fc) # Classifier Layer Init\n",
    "        # checkpointing\n",
    "        if self.cfg.gradient_checkpoint:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        # re-init Top-K Encoder Layer\n",
    "        if self.cfg.reinit:\n",
    "            self.reinit_topk_layers()\n",
    "    \n",
    "    # Classifier Layer Init\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    # Re-Init Top-K Transformers Layer\n",
    "    def reinit_topk_layers(self):\n",
    "        \"\"\"\n",
    "        Re-initialize the last-k transformer layers.\n",
    "        Args:\n",
    "            model: The target transformer model.\n",
    "            num_layers: The number of layers to be re-initialized.\n",
    "        \"\"\"\n",
    "        self.model.encoder.layer[-self.cfg.num_reinit:].apply(self.model._init_weights) # model classÏóê ÏûàÎäîÍ±∞\n",
    "        \n",
    "    def forward(self, inputs: dict):\n",
    "        \"\"\"\n",
    "        outputs.last_hidden_stats with no pooling => Token-Level Task\n",
    "        \"\"\"\n",
    "        outputs = self.model(**inputs) # inputs from LECRDataset\n",
    "        embedding = outputs.last_hidden_state\n",
    "        output = self.fc(embedding).squeeze(-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ù Step 5. Model & Metric Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adversarial Weight Perturbation\n",
    "\"\"\"\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        awp: bool,\n",
    "        adv_param: str=\"weight\",\n",
    "        adv_lr: float=1.0,\n",
    "        adv_eps: float=0.01\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.awp = awp\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "    def attack_backward(self, inputs: dict, label):\n",
    "        with torch.cuda.amp.autocast(enabled=self.awp):\n",
    "            self._save()\n",
    "            self._attack_step()\n",
    "            y_preds = self.model(inputs)\n",
    "            adv_loss = self.criterion(\n",
    "                y_preds.view(-1, 1), label.view(-1, 1))\n",
    "            mask = (label.view(-1, 1) != -1)\n",
    "            adv_loss = torch.masked_select(adv_loss, mask).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "        return adv_loss\n",
    "\n",
    "    def _attack_step(self) -> None:\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(\n",
    "                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "\n",
    "    def _save(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count        \n",
    "        \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps # If MSE == 0, We need eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "    \n",
    "def collate(inputs: dict) -> dict:\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "\n",
    "# Pearson Correlations Coeffiicient Loss\n",
    "class PearsonLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true) -> float:\n",
    "        x = y_pred.clone()\n",
    "        y = y_true.clone()\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "        cov = torch.sum(vx * vy)\n",
    "        corr = cov / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + 1e-12)\n",
    "        corr = torch.maximum(torch.minimum(corr,torch.tensor(1)), torch.tensor(-1))\n",
    "        \n",
    "        return torch.sub(torch.tensor(1), corr ** 2)\n",
    "    \n",
    "\n",
    "# F2-Score for each fold\n",
    "# Multi Classification => Positive Case would be highlighted, So We need to use Recall to Eval Metric\n",
    "\"\"\"\n",
    "Ïò§Ï∞®ÌñâÎ†¨ Î∞è Í¥ÄÎ†® ÏßÄÌëú Ï†ïÎ¶¨\n",
    "1) Î™®Îì†Í±¥ Ìï≠ÏÉÅ ÏòàÏ∏°Í∞í Í∏∞Ï§ÄÏúºÎ°ú ÏÉùÍ∞Å\n",
    "    - ÏòàÏ∏°Í∞íÏù¥ Positive && Ïã§Ï†ú Ï†ïÎãµÏù∏ Í≤ΩÏö∞ => True Positive, TP\n",
    "    - ÏòàÏ∏°Í∞íÏù¥ Positive && Ïã§Ï†ú Ïò§ÎãµÏù∏ Í≤ΩÏö∞ => False Positive, FP\n",
    "    - ÏòàÏ∏°Í∞íÏù¥ Negative && Ïã§Ï†ú Ïò§ÎãµÏù∏ Í≤ΩÏö∞ => True Negative, TN\n",
    "    - ÏòàÏ∏°Í∞íÏù¥ Negative && Ïã§Ï†ú Ï†ïÎãµÏù∏ Í≤ΩÏö∞ ==> False Negative, FN\n",
    "\n",
    "2) Precision\n",
    "    - precision = TP / (TP + FP)\n",
    "      => Ï†ïÎãµÏúºÎ°ú ÏòàÏ∏°Ìïú Í≤É Ï§ëÏóêÏÑú Ïã§Ï†ú Ï†ïÎãµÏóê Ìï¥Îãπ ÌïòÎäî Í≤ΩÏö∞\n",
    "    - Recall = TP / (TP + FN)\n",
    "      => Ïã§Ï†ú Ï†ïÎãµ Ï§ëÏóêÏÑú Î™®Îç∏Ïù¥ ÏòàÏ∏° ÏÑ±Í≥µÌïú Í≤ÉÏù¥ Î™áÍ∞ú Ïù∏Í∞Ä\n",
    "\"\"\"\n",
    "# def pearson_score(y_true, y_pred) -> float:\n",
    "#     return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "def pearson_score(y_true, y_pred) -> float:\n",
    "    x, y = y_pred, y_true\n",
    "    vx = x - np.mean(x)\n",
    "    vy = y - np.mean(y)\n",
    "    cov = np.sum(vx * vy)\n",
    "    corr = cov / (np.sqrt(np.sum(vx ** 2)) * np.sqrt(np.sum(vy ** 2)) + 1e-12)\n",
    "    return corr\n",
    "    \n",
    "def recall(y_true, y_pred) -> float:\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    recall = tp / (tp + fn)\n",
    "    return round(recall.mean(), 4)\n",
    "\n",
    "def precision(y_true, y_pred) -> float:\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    return round(precision.mean(), 4)\n",
    "\n",
    "def f2_score(y_true, y_pred) -> float:\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2.8 Trainer Input Class\n",
    "# Later append for AWP, SWA, Re-Init Code\n",
    "class TrainInput():\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.df = df # return dataset\n",
    "        if self.cfg.gradient_checkpoint:\n",
    "            self.save_parameter = f'(best_score) {self.cfg.model_name}_state_dict.pth' # checkpoint\n",
    "        \n",
    "    \n",
    "    # LLRD \n",
    "    def get_optimizer_grouped_parameters(self, model, layerwise_lr, layerwise_weight_decay, layerwise_lr_decay):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        # initialize lr for task specific layer\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "                                         \"weight_decay\": 0.0,\n",
    "                                         \"lr\": layerwise_lr,\n",
    "                                        },]\n",
    "        # initialize lrs for every layer\n",
    "        layers = [model.model.embeddings] + list(model.model.encoder.layer)\n",
    "        layers.reverse()\n",
    "        lr = layerwise_lr\n",
    "        for layer in layers:\n",
    "            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": layerwise_weight_decay,\n",
    "                                              \"lr\": lr,\n",
    "                                             },\n",
    "                                             {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": 0.0,\n",
    "                                              \"lr\": lr,\n",
    "                                             },]\n",
    "            lr *= layerwise_lr_decay\n",
    "        return optimizer_grouped_parameters\n",
    "    \n",
    "    def get_optimizer_params(self, model, encoder_lr, decoder_lr, weight_decay):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "\n",
    "    def make_batch(self, fold: int):\n",
    "        train = self.df[self.df['fold'] != fold].reset_index(drop=True)\n",
    "        valid = self.df[self.df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Custom Dataset\n",
    "        train_dataset = UPPPMDataset(self.cfg, train)\n",
    "        valid_dataset = UPPPMDataset(self.cfg, valid, is_valid=True)\n",
    "        valid_labels = valid['scores'].explode().to_numpy()\n",
    "\n",
    "        # DataLoader\n",
    "        loader_train = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size = self.cfg.batch_size,\n",
    "            shuffle = True,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "            num_workers = self.cfg.num_workers,\n",
    "            pin_memory = True,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        \n",
    "        loader_valid = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size = self.cfg.batch_size,\n",
    "            shuffle = False,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "            num_workers = self.cfg.num_workers,\n",
    "            pin_memory = True,\n",
    "            drop_last = False,\n",
    "        )\n",
    "        \n",
    "        return loader_train, loader_valid, train, valid, valid_labels\n",
    "\n",
    "    def model_setting(self):\n",
    "        \"\"\"\n",
    "        [model]\n",
    "        1) Re-Initialze Weights of Encoder\n",
    "           - DeBERTa => Last Two Layers == EMD        \n",
    "        2) SWA\n",
    "           - original model => to.device\n",
    "           - after calculate, update swa_model\n",
    "        \"\"\"\n",
    "        model = UPPPMModel(self.cfg, n_vocabs=len(self.tokenizer))        \n",
    "        #model.load_state_dict(torch.load('Token_Classification_Fold0_DeBERTa_V3_Large.pth'))\n",
    "        model.to(self.cfg.device)\n",
    "        # SWA: Stochastic Weighted Averaging        \n",
    "        if self.cfg.swa:\n",
    "            swa_model = AveragedModel(model)\n",
    "        else:\n",
    "            swa_model = 'none'\n",
    "        \n",
    "        # Setting Loss_Function\n",
    "        # Because we don't need to calculate none-target token: Output will be same shape as input\n",
    "        if self.cfg.loss_fn == 'BCE':\n",
    "            criterion = nn.BCEWithLogitsLoss(reduction='none') \n",
    "        if self.cfg.loss_fn == 'cross_entropy':\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        if self.cfg.loss_fn == 'pearson':\n",
    "            criterion = PearsonLoss()\n",
    "        if self.cfg.loss_fn == 'RMSE':\n",
    "            criterion = RMSELoss()\n",
    "            \n",
    "        # optimizer\n",
    "        grouped_optimizer_params = self.get_optimizer_grouped_parameters(\n",
    "            model, \n",
    "            self.cfg.layerwise_lr, \n",
    "            self.cfg.layerwise_weight_decay, \n",
    "            self.cfg.layerwise_lr_decay\n",
    "        )\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            grouped_optimizer_params,\n",
    "            lr = self.cfg.layerwise_lr,\n",
    "            eps = self.cfg.layerwise_adam_epsilon,\n",
    "            correct_bias = not self.cfg.layerwise_use_bertadam)\n",
    "        \n",
    "        return model, swa_model, criterion, optimizer, self.save_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3.1 Train & Validation Function\n",
    "def train_fn(cfg,\n",
    "             loader_train,\n",
    "             loader_valid,\n",
    "             model,\n",
    "             criterion,\n",
    "             optimizer,\n",
    "             scheduler,\n",
    "             valid,\n",
    "             valid_labels,\n",
    "             epoch,\n",
    "             swa_model = None,\n",
    "             swa_start = None,\n",
    "             swa_scheduler = None,):\n",
    "    # Train Stages\n",
    "    # torch.amp.gradscaler\n",
    "    awp = AWP(model, criterion, optimizer, cfg.awp, adv_lr=cfg.awp_lr, adv_eps=cfg.awp_eps)\n",
    "    if cfg.amp_scaler:\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    global_step, score_list = 0, [] # All Fold's average of mean F2-Score\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    for step, (inputs, _, labels) in enumerate(tqdm(loader_train)):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(cfg.device) # train to gpu\n",
    "        labels = labels.to(cfg.device) # label to gpu\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = cfg.amp_scaler):\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds.view(-1, 1), labels.view(-1,1))\n",
    "            mask = (labels.view(-1, 1) != -1)\n",
    "            loss = torch.masked_select(loss, mask).mean() # reduction = mean\n",
    "            losses.update(loss, batch_size)\n",
    "            \"\"\"\n",
    "            [gradient_accumlation]\n",
    "            - GPU VRAM OVER Î¨∏Ï†úÌï¥Í≤∞ÏùÑ ÏúÑÌï¥ ÏÇ¨Ïö©\n",
    "            - epochÏù¥ ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÏóêÌè≠ ÌöüÏàòÎ•º ÎÑòÏùÑ ÎïåÍπåÏßÄ Backward ÌïòÏßÄ ÏïäÍ≥† Í∑∏ÎùºÎîîÏñ∏Ìä∏ Ï∂ïÏ†Å\n",
    "            - ÏßÄÏ†ï epoch ÎÑòÏñ¥Í∞ÄÎ©¥ Ìïú Î≤àÏóê Backward\n",
    "            \"\"\"\n",
    "            if cfg.n_gradient_accumulation_steps > 1:\n",
    "                loss = loss / cfg.n_gradient_accumulation_steps\n",
    "                \n",
    "        scaler.scale(loss).backward()        \n",
    "        \"\"\"\n",
    "        [Adversarial Weight Training]\n",
    "        \"\"\"\n",
    "        if cfg.awp and epoch >= cfg.nth_awp_start_epoch:\n",
    "            loss = awp.attack_backward(inputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            awp._restore()\n",
    "        \"\"\"\n",
    "        1) Clipping Gradient && Gradient Accumlation\n",
    "        2) Stochastic Weight Averaging\n",
    "        \"\"\"\n",
    "        if cfg.clipping_grad and ((step + 1) % cfg.n_gradient_accumulation_steps == 0 or cfg.n_gradient_accumulation_steps == 1):\n",
    "            scaler.unscale_(optimizer)      \n",
    "            grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "                model.parameters(),\n",
    "                cfg.max_grad_norm\n",
    "            )\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if epoch >= int(swa_start):\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step()  \n",
    "            global_step += 1\n",
    "            scheduler.step()\n",
    "            \n",
    "    # Validation Stage\n",
    "    preds_list, label_list = [], []\n",
    "    valid_losses = AverageMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (inputs, target_masks, labels) in enumerate(tqdm(loader_valid)):\n",
    "            inputs = collate(inputs)\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(cfg.device)\n",
    "            labels = labels.to(cfg.device)\n",
    "            batch_size = labels.size(0)\n",
    "            preds = model(inputs)\n",
    "            valid_loss = criterion(preds.view(-1, 1), labels.view(-1, 1))\n",
    "            mask = (labels.view(-1, 1) != -1)\n",
    "            valid_loss = torch.masked_select(valid_loss, mask).mean()\n",
    "            valid_losses.update(valid_loss, batch_size)\n",
    "\n",
    "            y_preds = preds.sigmoid().to('cpu').numpy()\n",
    "                                          \n",
    "            anchorwise_preds = []\n",
    "            for pred, target_mask, in zip(y_preds, target_masks):\n",
    "                prev_i = -1\n",
    "                targetwise_pred_scores = []\n",
    "                for i, (p, tm) in enumerate(zip(pred, target_mask)):\n",
    "                    if tm != 0:\n",
    "                        if i-1 == prev_i:\n",
    "                            targetwise_pred_scores[-1].append(p)\n",
    "                        else:\n",
    "                            targetwise_pred_scores.append([p])\n",
    "                        prev_i = i\n",
    "                for targetwise_pred_score in targetwise_pred_scores:\n",
    "                    anchorwise_preds.append(np.mean(targetwise_pred_score))\n",
    "            preds_list.append(anchorwise_preds)\n",
    "    # error_list = [[i, preds_list.index(i)] for i in preds_list if i == 'nan' or i == float('inf')]\n",
    "    # print(error_list)\n",
    "    epoch_score = pearson_score(valid_labels, np.array(reduce(lambda a, b: a + b, preds_list)))\n",
    "    return losses.avg, valid_losses.avg, epoch_score, grad_norm, scheduler.get_lr()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swa_valid(cfg,\n",
    "              loader_valid,\n",
    "              swa_model,\n",
    "              criterion,\n",
    "              valid_labels):\n",
    "    swa_preds_list, swa_label_list = [], []\n",
    "    swa_model.eval()\n",
    "    swa_valid_losses = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (swa_inputs, target_masks, swa_labels) in enumerate(tqdm(loader_valid)):\n",
    "            swa_inputs = collate(swa_inputs)\n",
    "            \n",
    "            for k, v in swa_inputs.items():\n",
    "                swa_inputs[k] = v.to(cfg.device)\n",
    "                \n",
    "            swa_labels = swa_labels.to(cfg.device)\n",
    "            batch_size = swa_labels.size(0)\n",
    "            \n",
    "            swa_preds = swa_model(swa_inputs)\n",
    "            \n",
    "            swa_valid_loss = criterion(swa_preds.view(-1, 1), swa_labels.view(-1, 1))\n",
    "            mask = (swa_labels.view(-1, 1) != -1)\n",
    "            swa_valid_loss = torch.masked_select(swa_valid_loss, mask)\n",
    "            swa_valid_loss = swa_valid_loss.mean()\n",
    "            swa_valid_losses.update(swa_valid_loss, batch_size)\n",
    "            \n",
    "            swa_y_preds = swa_preds.sigmoid().to('cpu').numpy()\n",
    "                                          \n",
    "            anchorwise_preds = []\n",
    "            for pred, target_mask, in zip(swa_y_preds, target_masks):\n",
    "                prev_i = -1\n",
    "                targetwise_pred_scores = []\n",
    "                for i, (p, tm) in enumerate(zip(pred, target_mask)):\n",
    "                    if tm != 0:\n",
    "                        if i-1 == prev_i:\n",
    "                            targetwise_pred_scores[-1].append(p)\n",
    "                        else:\n",
    "                            targetwise_pred_scores.append([p])\n",
    "                        prev_i = i\n",
    "                for targetwise_pred_score in targetwise_pred_scores:\n",
    "                    anchorwise_preds.append(np.mean(targetwise_pred_score))\n",
    "                    \n",
    "            swa_preds_list.append(anchorwise_preds)\n",
    "    swa_valid_score = pearson_score(valid_labels, np.array(reduce(lambda a, b: a + b, swa_preds_list)))    \n",
    "    del swa_preds_list, swa_y_preds, swa_labels, anchorwise_preds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return swa_valid_losses.avg, swa_valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqcqced\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ce4a4d223c4dbcba1d721a76f4b03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666878751666445, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qcqced/Î∞îÌÉïÌôîÎ©¥/ML_Test/UPPPM/wandb/run-20230331_231942-exddeib5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification/runs/exddeib5' target=\"_blank\">[Append Version 2.2]Fold 0microsoft/deberta-v3-large</a></strong> to <a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification' target=\"_blank\">https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification/runs/exddeib5' target=\"_blank\">https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification/runs/exddeib5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Retriever Model :microsoft/deberta-v3-large =========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3d4fcdd09f4b3b92c7bd90eb95a90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 1th Fold Train & Validation ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762cd2bb0d744a1fba4c5198b442ba74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9513a0ade04bd1bd5d7981af8aa445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/180] Train Loss: 0.6843000054359436\n",
      "[1/180] Valid Loss: 0.6578999757766724\n",
      "[1/180] Pearson Score: 0.4097\n",
      "[1/180] Gradient Norm: 1780.4930419921875\n",
      "[1/180] lr: 2.7122321670735012e-06\n",
      "[Update] Valid Score : (-inf => 0.4097) Save Parameter\n",
      "Best Score: 0.40971906912197553\n",
      "[2/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537bb92b0795468da3adb350dcdfcf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f2339b40ff4851ac0d7d42f8174cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/180] Train Loss: 0.6531999707221985\n",
      "[2/180] Valid Loss: 0.6401000022888184\n",
      "[2/180] Pearson Score: 0.532\n",
      "[2/180] Gradient Norm: 2407.181640625\n",
      "[2/180] lr: 5.4244643341470025e-06\n",
      "[Update] Valid Score : (0.4097 => 0.5320) Save Parameter\n",
      "Best Score: 0.5319613917782015\n",
      "[3/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae469b0ac264239b9fabae62664c2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e59c5a8b2045728838fc1fee255847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/180] Train Loss: 0.640500009059906\n",
      "[3/180] Valid Loss: 0.6269999742507935\n",
      "[3/180] Pearson Score: 0.6087\n",
      "[3/180] Gradient Norm: 3722.11279296875\n",
      "[3/180] lr: 8.136696501220503e-06\n",
      "[Update] Valid Score : (0.5320 => 0.6087) Save Parameter\n",
      "Best Score: 0.608674825045642\n",
      "[4/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d2db444478420ca103db62ebf49a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca7087abeb6411b8fa03c822407e0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/180] Train Loss: 0.6287000179290771\n",
      "[4/180] Valid Loss: 0.6136999726295471\n",
      "[4/180] Pearson Score: 0.6612\n",
      "[4/180] Gradient Norm: 8064.52001953125\n",
      "[4/180] lr: 1.0848928668294005e-05\n",
      "[Update] Valid Score : (0.6087 => 0.6612) Save Parameter\n",
      "Best Score: 0.661183444323874\n",
      "[5/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25537e4d8b664624ab9640173f4973dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18192985e2444b96bdccd5f3143b5e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/180] Train Loss: 0.6150000095367432\n",
      "[5/180] Valid Loss: 0.5963000059127808\n",
      "[5/180] Pearson Score: 0.6958\n",
      "[5/180] Gradient Norm: 8425.140625\n",
      "[5/180] lr: 1.3561160835367507e-05\n",
      "[Update] Valid Score : (0.6612 => 0.6958) Save Parameter\n",
      "Best Score: 0.6957966041860348\n",
      "[6/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d72ef8259d4e4193fe97843922e474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c38f092ee44e26b6282bd08957d31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/180] Train Loss: 0.5999000072479248\n",
      "[6/180] Valid Loss: 0.5839999914169312\n",
      "[6/180] Pearson Score: 0.7249\n",
      "[6/180] Gradient Norm: 4018.826904296875\n",
      "[6/180] lr: 1.6273393002441007e-05\n",
      "[Update] Valid Score : (0.6958 => 0.7249) Save Parameter\n",
      "Best Score: 0.724850180184517\n",
      "[7/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ae85c110a14ccd865a397e41a96e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659aa570cb964c16bfcea42aef1151dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/180] Train Loss: 0.5882999897003174\n",
      "[7/180] Valid Loss: 0.5703999996185303\n",
      "[7/180] Pearson Score: 0.7546\n",
      "[7/180] Gradient Norm: 5371.4365234375\n",
      "[7/180] lr: 1.898562516951451e-05\n",
      "[Update] Valid Score : (0.7249 => 0.7546) Save Parameter\n",
      "Best Score: 0.7545719150912469\n",
      "[8/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e9a8e3ed444fd9a7fae7b9f10e2a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d3122bed1d43ce9524fdf9befe56ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/180] Train Loss: 0.5796999931335449\n",
      "[8/180] Valid Loss: 0.5673999786376953\n",
      "[8/180] Pearson Score: 0.7706\n",
      "[8/180] Gradient Norm: 6399.50830078125\n",
      "[8/180] lr: 2.169785733658801e-05\n",
      "[Update] Valid Score : (0.7546 => 0.7706) Save Parameter\n",
      "Best Score: 0.7706341882087255\n",
      "[9/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d9e46e8b0a4954ae1be0df919c6b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2561aa006cc0403798acf13151cb7bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/180] Train Loss: 0.5719000101089478\n",
      "[9/180] Valid Loss: 0.559499979019165\n",
      "[9/180] Pearson Score: 0.7849\n",
      "[9/180] Gradient Norm: 6250.1083984375\n",
      "[9/180] lr: 2.4410089503661513e-05\n",
      "[Update] Valid Score : (0.7706 => 0.7849) Save Parameter\n",
      "Best Score: 0.7848926436915169\n",
      "[10/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d56ecc7bef4679bdf15f526db1bb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f1c2f13e57452f80363bc4e1ae3e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/180] Train Loss: 0.5637999773025513\n",
      "[10/180] Valid Loss: 0.555899977684021\n",
      "[10/180] Pearson Score: 0.7985\n",
      "[10/180] Gradient Norm: 6449.2470703125\n",
      "[10/180] lr: 2.7122321670735013e-05\n",
      "[Update] Valid Score : (0.7849 => 0.7985) Save Parameter\n",
      "Best Score: 0.7985043920576239\n",
      "[11/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b6ef05f5094efc8899324079761bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d73482159a4c94af6bfc3cf046694b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/180] Train Loss: 0.5575000047683716\n",
      "[11/180] Valid Loss: 0.5504000186920166\n",
      "[11/180] Pearson Score: 0.8068\n",
      "[11/180] Gradient Norm: 5355.22265625\n",
      "[11/180] lr: 2.9834553837808517e-05\n",
      "[Update] Valid Score : (0.7985 => 0.8068) Save Parameter\n",
      "Best Score: 0.8067804918261992\n",
      "[12/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb60ab167a14aa28200ed7bc74e4769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e69f7b5a8e148d19e75a76d30b08f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/180] Train Loss: 0.5526000261306763\n",
      "[12/180] Valid Loss: 0.5532000064849854\n",
      "[12/180] Pearson Score: 0.813\n",
      "[12/180] Gradient Norm: 4369.27392578125\n",
      "[12/180] lr: 3.254678600488201e-05\n",
      "[Update] Valid Score : (0.8068 => 0.8130) Save Parameter\n",
      "Best Score: 0.8129845930798877\n",
      "[13/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7824fadb8a5e4df38a5444fc5991ac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc8e2cac6a74caba3a50aac915734a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/180] Train Loss: 0.5468999743461609\n",
      "[13/180] Valid Loss: 0.5467000007629395\n",
      "[13/180] Pearson Score: 0.8207\n",
      "[13/180] Gradient Norm: 3543.90869140625\n",
      "[13/180] lr: 3.525901817195551e-05\n",
      "[Update] Valid Score : (0.8130 => 0.8207) Save Parameter\n",
      "Best Score: 0.8207410904728175\n",
      "[14/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f04f01c4e834486af97c6afd7eabc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238e42d4fc8c4577bb390a68615239aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/180] Train Loss: 0.5428000092506409\n",
      "[14/180] Valid Loss: 0.5479000210762024\n",
      "[14/180] Pearson Score: 0.8222\n",
      "[14/180] Gradient Norm: 4250.6513671875\n",
      "[14/180] lr: 3.797125033902902e-05\n",
      "[Update] Valid Score : (0.8207 => 0.8222) Save Parameter\n",
      "Best Score: 0.8221954399992328\n",
      "[15/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62716cd09664705a749f26b68195c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63481501d30843ab8561b7405ee2ed72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/180] Train Loss: 0.5389000177383423\n",
      "[15/180] Valid Loss: 0.5475000143051147\n",
      "[15/180] Pearson Score: 0.8251\n",
      "[15/180] Gradient Norm: 2680.077392578125\n",
      "[15/180] lr: 4.068348250610252e-05\n",
      "[Update] Valid Score : (0.8222 => 0.8251) Save Parameter\n",
      "Best Score: 0.8250705062382038\n",
      "[16/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9dfc6aff4048a5ba45f2a1968a1c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e859159aa8e40d7a6d18ee1f5b1702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/180] Train Loss: 0.5353999733924866\n",
      "[16/180] Valid Loss: 0.5485000014305115\n",
      "[16/180] Pearson Score: 0.827\n",
      "[16/180] Gradient Norm: 4286.13134765625\n",
      "[16/180] lr: 4.339571467317602e-05\n",
      "[Update] Valid Score : (0.8251 => 0.8270) Save Parameter\n",
      "Best Score: 0.8269675036996958\n",
      "[17/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b8876908e64f6aa404351cf93acff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6102c52af6457182c4aee93157efb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/180] Train Loss: 0.5324000120162964\n",
      "[17/180] Valid Loss: 0.5525000095367432\n",
      "[17/180] Pearson Score: 0.8294\n",
      "[17/180] Gradient Norm: 6413.98828125\n",
      "[17/180] lr: 4.610794684024952e-05\n",
      "[Update] Valid Score : (0.8270 => 0.8294) Save Parameter\n",
      "Best Score: 0.8294301481083621\n",
      "[18/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acfb9abd2f4476c80ebef9fa2e6c365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168e4d41ad8a4681802e8cc0cc9ea813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/180] Train Loss: 0.5307999849319458\n",
      "[18/180] Valid Loss: 0.545799970626831\n",
      "[18/180] Pearson Score: 0.8343\n",
      "[18/180] Gradient Norm: 2480.564208984375\n",
      "[18/180] lr: 4.8820179007323027e-05\n",
      "[Update] Valid Score : (0.8294 => 0.8343) Save Parameter\n",
      "Best Score: 0.834254476997923\n",
      "[19/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23d37e13781482fb0b3f1ca028365e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dc69b5fa444bcbb54656c1a38d9c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/180] Train Loss: 0.5299000144004822\n",
      "[19/180] Valid Loss: 0.5479000210762024\n",
      "[19/180] Pearson Score: 0.8337\n",
      "[19/180] Gradient Norm: 1635.8603515625\n",
      "[19/180] lr: 4.9634645331736365e-05\n",
      "[20/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9df5584955441599b7b641f05f4185d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f95e95f141543f1a15b844c1d9b5eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/180] Train Loss: 0.5249999761581421\n",
      "[20/180] Valid Loss: 0.5479000210762024\n",
      "[20/180] Pearson Score: 0.8362\n",
      "[20/180] Gradient Norm: 3918.79931640625\n",
      "[20/180] lr: 4.724224290233164e-05\n",
      "[Update] Valid Score : (0.8343 => 0.8362) Save Parameter\n",
      "Best Score: 0.8361648769024356\n",
      "[21/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ee9884137e4fcd8e94b90d77a2558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4772093225449feb08253e6d392b74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/180] Train Loss: 0.5217000246047974\n",
      "[21/180] Valid Loss: 0.5519999861717224\n",
      "[21/180] Pearson Score: 0.8387\n",
      "[21/180] Gradient Norm: 2828.005859375\n",
      "[21/180] lr: 4.282391876428358e-05\n",
      "[Update] Valid Score : (0.8362 => 0.8387) Save Parameter\n",
      "Best Score: 0.8387472586881518\n",
      "[22/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2db6e4e51b43b4b47276c2847cfec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0f35c2264e458e8d5d596f93ad2117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/180] Train Loss: 0.5181999802589417\n",
      "[22/180] Valid Loss: 0.5485000014305115\n",
      "[22/180] Pearson Score: 0.8376\n",
      "[22/180] Gradient Norm: 3624.087890625\n",
      "[22/180] lr: 3.678211339207133e-05\n",
      "[23/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81a5f71307d40349be2e7c68fd34a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5c38b32eba4540989a2dbfdf223a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/180] Train Loss: 0.5163000226020813\n",
      "[23/180] Valid Loss: 0.5508000254631042\n",
      "[23/180] Pearson Score: 0.8412\n",
      "[23/180] Gradient Norm: 3202.439453125\n",
      "[23/180] lr: 2.9667141100519634e-05\n",
      "[Update] Valid Score : (0.8387 => 0.8412) Save Parameter\n",
      "Best Score: 0.8412145154352871\n",
      "[24/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99f32efff884104905854147074789f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ccde97ff7548fa959ee89083e9cf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/180] Train Loss: 0.5146999955177307\n",
      "[24/180] Valid Loss: 0.550000011920929\n",
      "[24/180] Pearson Score: 0.84\n",
      "[24/180] Gradient Norm: 3588.76953125\n",
      "[24/180] lr: 2.212706498673368e-05\n",
      "[25/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4ab6e4f4aa4b3b8d4cbb618869de76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc37616def6843b6b1e3b4f93c5fd248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/180] Train Loss: 0.51419997215271\n",
      "[25/180] Valid Loss: 0.555899977684021\n",
      "[25/180] Pearson Score: 0.8385\n",
      "[25/180] Gradient Norm: 5315.53369140625\n",
      "[25/180] lr: 1.48486684812703e-05\n",
      "[26/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089ea89e88ec4bf5b4a6cc11a41681e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4097936f2d74f3f94d3776262002f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/180] Train Loss: 0.5115000009536743\n",
      "[26/180] Valid Loss: 0.5515999794006348\n",
      "[26/180] Pearson Score: 0.8414\n",
      "[26/180] Gradient Norm: 4035.801513671875\n",
      "[26/180] lr: 8.494900080343715e-06\n",
      "[Update] Valid Score : (0.8412 => 0.8414) Save Parameter\n",
      "Best Score: 0.8414331603470655\n",
      "[27/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e3a6376c2b4dac9ab49ba7bac6cca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c1800dd2cb4a99872bfb9ea5b7d7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/180] Train Loss: 0.511900007724762\n",
      "[27/180] Valid Loss: 0.5548999905586243\n",
      "[27/180] Pearson Score: 0.8409\n",
      "[27/180] Gradient Norm: 4706.0908203125\n",
      "[27/180] lr: 3.6444890691097577e-06\n",
      "[28/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024c400c46b944318fbd34ae060c71b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3d9b1321834b219026a243a57eb9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/180] Train Loss: 0.5116999745368958\n",
      "[28/180] Valid Loss: 0.5547999739646912\n",
      "[28/180] Pearson Score: 0.8408\n",
      "[28/180] Gradient Norm: 2299.37841796875\n",
      "[28/180] lr: 7.392323026181453e-07\n",
      "[29/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0a3138d5884e0eb8bb1060c2925040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26c4bceecfd4e49b8dd2a392d634791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/180] Train Loss: 0.5105999708175659\n",
      "[29/180] Valid Loss: 0.5533999800682068\n",
      "[29/180] Pearson Score: 0.8417\n",
      "[29/180] Gradient Norm: 3829.83349609375\n",
      "[29/180] lr: 4.995624660278707e-05\n",
      "[Update] Valid Score : (0.8414 => 0.8417) Save Parameter\n",
      "Best Score: 0.8417452063837799\n",
      "[30/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a88917f5954cd980d2a5ca1f17dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8756d6c37d0c4ac78bac2fd7f17db43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/180] Train Loss: 0.5120000243186951\n",
      "[30/180] Valid Loss: 0.5551999807357788\n",
      "[30/180] Pearson Score: 0.8417\n",
      "[30/180] Gradient Norm: 3008.890380859375\n",
      "[30/180] lr: 4.8378600357061824e-05\n",
      "[31/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0716beb72f447aa8c1daea52e19f20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79445438e10a41059d4ce724c71d0fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/180] Train Loss: 0.5110999941825867\n",
      "[31/180] Valid Loss: 0.5514000058174133\n",
      "[31/180] Pearson Score: 0.8404\n",
      "[31/180] Gradient Norm: 1851.47314453125\n",
      "[31/180] lr: 4.4671527947390544e-05\n",
      "[32/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557d1bd203df45beb4d7d084b6561243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6fb71ba2924606a40e7923f83a3f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/180] Train Loss: 0.5120000243186951\n",
      "[32/180] Valid Loss: 0.5611000061035156\n",
      "[32/180] Pearson Score: 0.8379\n",
      "[32/180] Gradient Norm: 4354.42138671875\n",
      "[32/180] lr: 3.917268589983689e-05\n",
      "[33/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f99310216ba48cbb60d5153400c80b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a8ccbd63d7461eb8b52d45d0e7bb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/180] Train Loss: 0.5091000199317932\n",
      "[33/180] Valid Loss: 0.5569000244140625\n",
      "[33/180] Pearson Score: 0.8416\n",
      "[33/180] Gradient Norm: 8824.7109375\n",
      "[33/180] lr: 3.23829330318078e-05\n",
      "[34/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5f2279e11c4a878444f96cd5eac8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7029249c050467e962f0ff92e573a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/180] Train Loss: 0.5092999935150146\n",
      "[34/180] Valid Loss: 0.5547000169754028\n",
      "[34/180] Pearson Score: 0.8418\n",
      "[34/180] Gradient Norm: 2301.409912109375\n",
      "[34/180] lr: 2.4920710019096003e-05\n",
      "[Update] Valid Score : (0.8417 => 0.8418) Save Parameter\n",
      "Best Score: 0.841761358055445\n",
      "[35/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e983b1490e4c3e80dd29a33eedc957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is Test Code for Appending Cross-Validaton Strategy (fold to epoch)\n",
    "cfg_list = [CFG]\n",
    "for cfg in cfg_list:\n",
    "    #init wandb\n",
    "    wandb.init(project=\"[Append Ver 2]UPPPM Token Classification\", \n",
    "               name='[Append Version 2.2]' + 'Fold 0' + cfg.model_name,\n",
    "               config=class2dict(cfg),\n",
    "               group=cfg.model_name,\n",
    "               job_type=\"train\",\n",
    "               entity = \"qcqced\")\n",
    "    wandb_config = wandb.config\n",
    "    print(f'========================= Retriever Model :{cfg.model_name} =========================')\n",
    "    fold_list, swa_score_max = [i for i in range(cfg.n_folds)], -np.inf\n",
    "    \n",
    "    for fold in tqdm(fold_list):\n",
    "        print(f'============== {fold+1}th Fold Train & Validation ==============')\n",
    "        val_score_max = -np.inf\n",
    "        fold_train_loss_list, fold_valid_loss_list, fold_score_list  = [], [], []\n",
    "        fold_swa_loss, fold_swa_score = [], []\n",
    "        \n",
    "        train_input = TrainInput(cfg, train_df) # init object\n",
    "        model, swa_model, criterion, optimizer, save_parameter = train_input.model_setting()\n",
    "        loader_train, loader_valid, train, valid, valid_labels = train_input.make_batch(fold)\n",
    "        \n",
    "        # Scheduler Setting\n",
    "        if cfg.swa:\n",
    "            swa_start = cfg.swa_start\n",
    "            swa_scheduler = SWALR(\n",
    "                optimizer,\n",
    "                swa_lr = cfg.swa_lr, # Later Append\n",
    "                anneal_epochs=cfg.anneal_epochs, \n",
    "                anneal_strategy=cfg.anneal_strategy\n",
    "            )\n",
    "            \n",
    "        if cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps) * cfg.warmup_ratio,\n",
    "                num_training_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps),\n",
    "                num_cycles = cfg.num_cycles\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine_annealing':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps) * cfg.warmup_ratio,\n",
    "                num_training_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps),\n",
    "                num_cycles = 8 \n",
    "            )\n",
    "        else:\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs) * cfg.warmup_ratio,\n",
    "                num_training_steps=int(len(train) /cfg.batch_size * cfg.epochs),\n",
    "                num_cycles = cfg.num_cycles\n",
    "            )  \n",
    "\n",
    "        for epoch in range(cfg.epochs):\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Train & Validation')\n",
    "            if cfg.swa:\n",
    "                train_loss, valid_loss, score, grad_norm, lr = train_fn(\n",
    "                    cfg,\n",
    "                    loader_train,\n",
    "                    loader_valid,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    valid,\n",
    "                    valid_labels,\n",
    "                    int(epoch),\n",
    "                    swa_model=swa_model,\n",
    "                    swa_start=swa_start,\n",
    "                    swa_scheduler=swa_scheduler,\n",
    "            )        \n",
    "            else:\n",
    "                train_loss, valid_loss, score = train_fn(\n",
    "                    cfg,\n",
    "                    loader_train,\n",
    "                    loader_valid,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    valid,\n",
    "                    valid_labels,\n",
    "                    int(epoch),\n",
    "            )\n",
    "            \n",
    "            train_loss = train_loss.detach().cpu().numpy()\n",
    "            valid_loss = valid_loss.detach().cpu().numpy()\n",
    "            grad_norm = grad_norm.detach().cpu().numpy()\n",
    "            \n",
    "            fold_train_loss_list.append(train_loss)\n",
    "            fold_valid_loss_list.append(valid_loss)\n",
    "            fold_score_list.append(score)\n",
    "           \n",
    "            wandb.log({\n",
    "                '<epoch> Train Loss': train_loss,\n",
    "                '<epoch> Valid Loss': valid_loss,\n",
    "                '<epoch> Pearson_Score': score,\n",
    "                '<epoch> Gradient Norm': grad_norm,\n",
    "                '<epoch> lr': lr\n",
    "            })\n",
    "            \n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Train Loss: {np.round(train_loss, 4)}') \n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Valid Loss: {np.round(valid_loss, 4)}')\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Pearson Score: {np.round(score, 4)}')\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Gradient Norm: {np.round(grad_norm, 4)}')\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] lr: {lr}')\n",
    "            \n",
    "        \n",
    "            if val_score_max <= score:\n",
    "                print(f'[Update] Valid Score : ({val_score_max:.4f} => {score:.4f}) Save Parameter')\n",
    "                print(f'Best Score: {score}')\n",
    "                torch.save(model.state_dict(),\n",
    "                           f'Ver2-3_Token_Classification_Fold{fold}_DeBERTa_V3_Large.pth')\n",
    "                val_score_max = score\n",
    "            \n",
    "        del train_loss, valid_loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f'================= {fold+1}th Train & Validation =================')            \n",
    "        fold_train_loss = np.mean(fold_train_loss_list)\n",
    "        fold_valid_loss = np.mean(fold_valid_loss_list)\n",
    "        fold_score = np.mean(fold_score_list)\n",
    "        wandb.log({f'<Fold{fold+1}> Train Loss': fold_train_loss,\n",
    "                   f'<Fold{fold+1}> Valid Loss': fold_valid_loss,\n",
    "                   f'<Fold{fold+1}> Pearson_Score': fold_score,})\n",
    "        print(f'Fold[{fold+1}/{fold_list[-1]+1}] Train Loss: {np.round(fold_train_loss, 4)}')\n",
    "        print(f'Fold[{fold+1}/{fold_list[-1]+1}] Valid Loss: {np.round(fold_valid_loss, 4)}')\n",
    "        print(f'Fold[{fold+1}/{fold_list[-1]+1}] Pearson Score: {np.round(fold_score, 4)}')\n",
    "        \n",
    "        if cfg.swa:\n",
    "            update_bn(loader_train, swa_model) # Stochastic Weight Averaging\n",
    "            fold_swa_loss, fold_swa_score = swa_valid(\n",
    "                cfg,\n",
    "                loader_valid,\n",
    "                swa_model,\n",
    "                criterion,\n",
    "                valid_labels,\n",
    "            )\n",
    "            fold_swa_loss = fold_swa_loss.detach().cpu().numpy()\n",
    "            fold_swa_loss = np.mean(fold_swa_loss)\n",
    "            fold_swa_score = np.mean(fold_swa_score)\n",
    "            \n",
    "            wandb.log({\n",
    "                f'<Fold{fold+1}> SWA Valid Loss': fold_swa_loss,\n",
    "                f'<Fold{fold+1}> SWA Pearson_Score': fold_swa_score,\n",
    "            })\n",
    "            \n",
    "            print(f'Fold[{fold+1}/{fold_list[-1]+1}] SWA Loss: {np.round(fold_swa_loss, 4)}')\n",
    "            print(f'Fold[{fold+1}/{fold_list[-1]+1}] SWA Score: {np.round(fold_swa_score, 4)}') \n",
    "        \n",
    "        if val_score_max <= fold_swa_score:\n",
    "            print(f'[Update] Valid Score : ({val_score_max:.4f} => {fold_swa_score:.4f}) Save Parameter')\n",
    "            print(f'Best Score: {fold_swa_score}')\n",
    "            torch.save(model.state_dict(),\n",
    "                       f'SWA_Ver2-3_Token_Classification_Fold{fold}_DeBERTa_V3_Large.pth')\n",
    "            val_score_max = fold_score\n",
    "            \n",
    "        del fold_swa_loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Append Ver 1]\n",
    "1) Add Gradient Accumulation & Clipping Norm\n",
    "- ÎäêÎ¶¨ÏßÄÎßå ÌôïÏã§Ìûà ÏïàÏ†ïÏ†ÅÏúºÎ°ú Ïä§ÏΩîÏñ¥ ÏÉÅÏäπ, Îã§ÏùåÏóê Clipping Norm ÏÑ§Ï†ï ÎÅÑÍ≥† epochÎ≥Ñ Gradient Ï¥ùÎüâÏùÑ Íµ¨Ìï¥ÏÑú max_norm Í∞íÏùÑ accumulation step Ïà´ÏûêÏóê ÎßûÍ≤å ÏàòÏ†ïÌïòÏûê  \n",
    "- ÌôïÏã§Ìûà max_grad_normÏùÑ Ïò¨Î¶¨ÎãàÍπå Ï≤òÏùåÎ∂ÄÌÑ∞ Ïñ¥ÏßÄÍ∞ÑÌïú ÏÑ±Îä•Ïù¥ ÎÇòÏò§Îäî ÎìØ  \n",
    "- Running Time ÏÉùÍ∞Å ÏïàÌïòÍ≥† Batch & Epoch Í¥ÄÏ†êÏóêÏÑúÎßå Î≥¥Î©¥ Ìõ®Ïî¨ ÏÑ±Îä•Ïù¥ Ï¢ãÏïÑ ÏßÑ Í±∞Í∏¥ Ìï®  \n",
    "- learning rateÎèÑ accumulation Ïà´ÏûêÏóê ÎßûÍ≤å ÎÇòÎà†Ï§òÏïº ÌïòÎäîÍ±∞ ÏïÑÎãåÍ∞Ä?? ÌïúÎ≤à Ìï¥Î≥¥Ïûê\n",
    "\n",
    "2) Add Stochastic Weight Averaging  \n",
    "- OOM Î∞úÏÉù ÏïàÌïòÍ≥† Ïûò ÎèåÏïÑÍ∞ÑÎã§ \n",
    "- VRAMÏù¥ ÏßÑÏßú 10% Ï†ïÎèÑ ÏÉÅÏäπÌï® \n",
    "- Ïù¥Ï†ú Î¨∏Ï†úÎäî..... AWP  \n",
    "- epoch 8 ÏÑ§Ï†ïÌïòÍ≥† ÌïúÎ≤à ÏùºÎ∞ò ValidationÏù¥Îûë ÏñºÎßàÎÇò Ï∞®Ïù¥Í∞Ä ÎÇòÎäîÏßÄ ÌôïÏù∏Ìï¥Î≥¥Ïûê  \n",
    "  ÏùºÎ∞ò Î™®Îç∏ Ìè¥Îìú ÌèâÍ∑†: 0.51  \n",
    "  SWA Î™®Îç∏: 0.60  \n",
    "\n",
    "3) Adversarial Weight Pertubation  \n",
    "- token max_len = 400 => ÏïÑÏä¨ÏïÑÏä¨ÌïòÍ≤å ÎèåÏïÑÍ∞ê\n",
    "- Í∑ºÎç∞ ÌïôÏäµ ÏÜçÎèÑÍ∞Ä ÎÑàÎ¨¥ ÎäêÎ†§ÏÑú Î™ªÏç®Î®πÍ≤†Ïùå  \n",
    "- Ïù¥Í±∞ Í∞ôÏù¥ Ïì∏Í±∞Î©¥ Gradient NormÏùÑ Ïò¨Î¶¨Îçò ÌïôÏäµÎ•†ÏùÑ Ïò¨Î¶¨Îçò Ìï¥ÏïºÍ≤†Ïùå  \n",
    "\n",
    "4) Cosine Scheduler  \n",
    "- num_cycle: 8  \n",
    "  [Ï¥àÍ∏∞]  \n",
    "  epoch=180, num_accumulation = 4, num_cycle =2\n",
    "  Ïã§Ï†ú Ïä§ÏºÄÏ§ÑÎü¨ stepÏùÄ 45Ìöå Î∞úÏÉù, Ïã§Ìóò Í≤∞Í≥º ÏóêÌè≠ 60Ìöå(Ïã§ ÏóêÌè≠ 15Ìöå) Î∂ÄÌÑ∞ Ïä§ÏΩîÏñ¥Í∞Ä ÏàòÎ†¥ÌïòÎäî Í≤ΩÌñ•ÏÑ±ÏùÑ Î≥¥ÏûÑ  \n",
    "  num_cycle = 2Î°ú ÏÑ§Ï†ïÌñàÍ∏∞ ÎïåÎ¨∏Ïóê, ÏÇ¨Ïã§ÏÉÅ Ìïú Ï£ºÍ∏∞Ïùò Ï†àÎ∞òÏØ§Î∂ÄÌÑ∞ ÏàòÎ†¥Ïù¥ Î∞úÏÉùÌï®  \n",
    "  Îã®ÏàúÌïòÍ≤å ÏÉùÍ∞ÅÌï¥Î≥¥Î©¥ num_cycle * 2 * = 16ÏúºÎ°ú ÏÉàÎ°úÏö¥ ÌååÏù¥ÌîÑÎùºÏù∏Ïùò num_cycle ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏù¥ ÎßûÏïÑÎ≥¥Ïù¥ÏßÄÎßå,  \n",
    "  ÌïôÏäµÎ•†Ïùò Í∞êÏÜåÌïòÎäî Ï∂îÏÑ∏Î•º Í≥†Î†§ÌñàÏùÑÎïå 8Ï†ïÎèÑÎ°ú ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏù¥ ÏùºÎã® Ï¢ãÏïÑ Î≥¥Ïù∏Îã§.\n",
    "  Ïù¥Îü∞ Í≥ÑÏÇ∞ Î∞è Í∞ÄÏ†ïÌïòÎäî Í≤ÉÏù¥ Í∑ÄÏ∞ÆÏùÑ ÎøêÎçîÎü¨ Ï†ïÌôïÌïú ÌïôÏäµ Í≤∞Í≥ºÎ•º Î≥¥Ïû•ÌïòÎäî Í≤ÉÎèÑ ÏïÑÎãàÍ∏∞ ÎïåÎ¨∏Ïóê ÏñºÎ•∏ ÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù Ìà¥ÏùÑ Ï†ÅÏö©ÌïòÏûê  \n",
    "  \n",
    "[Append Ver 2]  \n",
    "1) num_accumulation = 1\n",
    "- Ïù¥Í±∞ Ï†ÅÏö©ÌïòÎäî Í≤ÉÏùò Ïù¥Ï†êÏù¥ Îî±Ìûà ÏóÜÎäîÎìØ  \n",
    "- AWP = False => max_len = 512\n",
    "2) Clipping Grad Norm = 1000  \n",
    "\n",
    "3) Cosine Annealing Scheduler  \n",
    "- num_cycle: 8, 16  \n",
    "\n",
    "4) Re-Init Top Encoder Block  \n",
    "- num_reinit: 5  \n",
    "\n",
    "5) Cross Validation  \n",
    "- num_fold: 4  \n",
    "  Ïã§Ï†ú ÎåÄÌöå LBÎäî 25%, PBÎäî 75%ÎùºÏÑú Ìï¥Îãπ ÎπÑÏú®Ïóê ÎßûÍ≤å Ìè¥ÎìúÎ•º Íµ¨ÏÑ±ÌïòÏûê...CVÏùò Ï§ëÏöîÏÑ±ÏùÄ ÏßÑÏßú 100000ÏñµÎ≤à Î∞òÎ≥µÌï¥ÎèÑ ÏßÄÎÇòÏπòÏßÄ ÏïäÏùÄ Í≤É Í∞ôÎã§  \n",
    "  \n",
    "6) Add LR Tracker  \n",
    "\n",
    "7) Append LR  \n",
    "- options: 1e-5, 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, model, swa_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
