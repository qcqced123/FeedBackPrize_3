{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of This Notebook  \n",
    "1) Want to experience tuning models' parameter & several strategy experiment  \n",
    "2) Experience Token Classification Task  \n",
    "3) Study Top Ranker's Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# !unzip /home/qcqced/nltk_data/corpora/wordnet.zip -d /home/qcqced/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.swa_utils as swa\n",
    "import tokenizers, transformers\n",
    "import os, sys, gc, time, random, warnings, math, re\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AdamW\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from numpy import ndarray\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
    "from tqdm.auto import tqdm\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "from glob import glob\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/qcqced/.netrc\n"
     ]
    }
   ],
   "source": [
    "# WandB Login => Copy API Key\n",
    "secret_value_0 = '8d7716caaaa5afb56e1d02ef5837cabbffe48b41'\n",
    "!wandb login $secret_value_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1 Configuration Setting\n",
    "\"\"\"\n",
    "[Configuration]\n",
    "    - Pooling: mean, attention, max, weightedlayer, concat (This Pipeline doesn't need to pooling)\n",
    "    - Optimizer: AdamW, SWA\n",
    "    - Scheduler: cosine, linear\n",
    "    - Clip_grad_norm, Gradient Checking: T/F\n",
    "    - LLRD\n",
    "    - Re-Init\n",
    "    - AWP\n",
    "\"\"\"\n",
    "class CFG:\n",
    "    \"\"\"--------[Common]--------\"\"\"\n",
    "    wandb, train, competition, seed, cfg_name = True, True, 'UPPPM', 42, 'CFG'\n",
    "    device, gpu_id = torch.device('cuda' if torch.cuda.is_available() else 'cpu'), 0\n",
    "    num_workers = 0\n",
    "    \"\"\" Mixed Precision, Gradient Check Point \"\"\"\n",
    "    amp_scaler = True\n",
    "    gradient_checkpoint = True # save parameter\n",
    "    output_dir = './output/'\n",
    "    \"\"\" Clipping Grad Norm, Gradient Accumulation \"\"\"\n",
    "    clipping_grad = True # clip_grad_norm\n",
    "    n_gradient_accumulation_steps = 1 # Gradient Accumulation\n",
    "    max_grad_norm = n_gradient_accumulation_steps * 1000\n",
    "    \"\"\" Model \"\"\"\n",
    "    model_name = 'microsoft/deberta-v3-large' \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#    pooling = 'attention'\n",
    "    max_len = 512\n",
    "    \"\"\" CV, Epoch, Batch Size \"\"\"\n",
    "    n_folds = 4\n",
    "    epochs = 180\n",
    "    batch_size = 64\n",
    "    \"\"\" SWA, Loss, Optimizer, Scheduler \"\"\"\n",
    "    swa = True\n",
    "    swa_start = int(epochs*0.75)\n",
    "    swa_lr = 1e-4\n",
    "    anneal_epochs = 4\n",
    "    anneal_strategy = 'cos' # default = cos, available option: linear \n",
    "    loss_fn = 'BCE'\n",
    "    optimizer = 'AdamW' # options: SWA, AdamW\n",
    "    weight_decay = 1e-2\n",
    "    scheduler = 'cosine_annealing' # options: cosine, linear, cosine_annealing, linearannealing\n",
    "    num_cycles = 0.5\n",
    "#    num_warmup_steps = 0\n",
    "    warmup_ratio = 0.1 # options: 0.05, 0.1\n",
    "    batch_scheduler = True\n",
    "    # encoder_lr = 5e-5\n",
    "    # decoder_lr = 1e-5\n",
    "    min_lr = 1e-7\n",
    "    # eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    \"\"\" LLRD \"\"\"\n",
    "    llrd = True\n",
    "    layerwise_lr = 5e-5\n",
    "    layerwise_lr_decay = 0.9\n",
    "    layerwise_weight_decay = 1e-2\n",
    "    layerwise_adam_epsilon = 1e-6\n",
    "    layerwise_use_bertadam = False\n",
    "    \"\"\" Re-Init, AWP \"\"\"\n",
    "    reinit = True\n",
    "    num_reinit = 5\n",
    "    awp = False\n",
    "    nth_awp_start_epoch = 10\n",
    "    awp_eps = 1e-2\n",
    "    awp_lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f245eefac70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_device() -> bool:\n",
    "    return torch.backends.mps.is_available()\n",
    "\n",
    "def check_library(checker: bool) -> tuple:\n",
    "    \"\"\"\n",
    "    1) checker == True \n",
    "        - current device is mps\n",
    "    2) checker == False\n",
    "        - current device is cuda with cudnn\n",
    "    \"\"\"\n",
    "    if not checker:\n",
    "        _is_built = torch.backends.cudnn.is_available()\n",
    "        _is_enable = torch.backends.cudnn.enabledtorch.backends.cudnn.enabled\n",
    "        version = torch.backends.cudnn.version()\n",
    "        device = (_is_built, _is_enable, version)\n",
    "        return device\n",
    "\n",
    "def class2dict(cfg: CFG) -> dict:\n",
    "    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))\n",
    "\n",
    "def all_type_seed(cfg: CFG, checker: bool) -> None:\n",
    "    # python & torch seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)  # python Seed\n",
    "    random.seed(cfg.seed)  # random module Seed\n",
    "    np.random.seed(cfg.seed)  # numpy module Seed\n",
    "    torch.manual_seed(cfg.seed)  # Pytorch CPU Random Seed Maker\n",
    "\n",
    "    # device == cuda\n",
    "    if not checker: \n",
    "        torch.cuda.manual_seed(cfg.seed)  # Pytorch GPU Random Seed Maker\n",
    "        torch.cuda.manual_seed_all(cfg.seed)  # Pytorch Multi Core GPU Random Seed Maker\n",
    "        # torch.cudnn seed\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # devide == mps\n",
    "#     else:\n",
    "#         torch.mps.manual_seed(cfg.seed)\n",
    "    \n",
    "def seed_worker(worker_id) -> None:\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "def get_logger(filename: str):\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "if not os.path.exists(CFG.output_dir):\n",
    "    os.makedirs(CFG.output_dir)\n",
    "\n",
    "logger = get_logger(filename=CFG.output_dir+'train')\n",
    "check_library(True)\n",
    "all_type_seed(CFG, True)    \n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ Step 1. Stratified Group K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Stratified Group K]\n",
    "ê°™ì€ ê·¸ë£¹ì€ ì„œë¡œ ê°™ì€ ì„±ì§ˆì˜ ë°ì´í„° ì„¸íŠ¸ì— í¬í•¨ë˜ë©°, ì „ì²´ ë¼ë²¨ ë¶„í¬ë¥¼ ê³ ë ¤í•œ í´ë“œ êµ¬ì„± ì „ëžµ\n",
    "1) ì ìˆ˜ë¥¼ í´ëž˜ìŠ¤ë¡œ ì¹˜í™˜\n",
    "    - 0.00: class 0\n",
    "    - 0.25: class 1\n",
    "    - 0.50: class 2\n",
    "    - 0.75: class 3\n",
    "    - 1.00: class 4\n",
    "2) ë¼ë²¨ ë¶„í¬ ê· í˜•ì„ ìœ„í•´ StratifiedGroupKFold ì‚¬ìš©\n",
    "3) Group ê¸°ì¤€ì€ 'anchor'\n",
    "4) ì›ìž‘ìžëŠ” í´ë“œë¥¼ 2ê°œë¡œ êµ¬ì„±í•˜ê³  í…ŒìŠ¤íŠ¸ë¥¼ ëŒë¦°ë“¯\n",
    "\"\"\"\n",
    "def cross_val(cfg):\n",
    "    train_df = pd.read_csv('./dataset/train.csv')\n",
    "    kfold = StratifiedGroupKFold(\n",
    "        n_splits=CFG.n_folds, \n",
    "        shuffle=True, \n",
    "        random_state=cfg.seed\n",
    "    )\n",
    "    train_df[\"score_class\"] = train_df[\"score\"].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n",
    "    train_df['fold'] = -1\n",
    "    for num, (tx, vx) in enumerate(kfold.split(train_df, train_df[\"score_class\"], train_df[\"anchor\"])):\n",
    "        train_df.loc[vx, \"fold\"] = num\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Stage 2. Text Preprocessing: Normalization & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"[Text Normalization]\"\"\"\n",
    "def create_word_normalizer() -> function:\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def normalize(word):\n",
    "        w = word.lower()\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "        w = ps.stem(w)\n",
    "        return w\n",
    "    return normalize\n",
    "\n",
    "def __normalize_words(titles: list) -> list:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    normalizer = create_word_normalizer()\n",
    "    titles = [normalizer(t) for t in titles if t not in stop_words]\n",
    "    return titles\n",
    "\n",
    "def normalize_words(words: ndarray, unique=True):\n",
    "    if type(words) is str:\n",
    "        words = [words]\n",
    "    sep_re = r'[\\s\\(\\){}\\[\\];,\\.]+'\n",
    "    num_re = r'\\d'\n",
    "    words = re.split(sep_re, ' '.join(words).lower())\n",
    "    words = [w for w in words if len(w) >= 3 and not re.match(num_re, w)]\n",
    "    if unique:\n",
    "        words = list(set(words))\n",
    "        words = set(__normalize_words(words))\n",
    "    else:\n",
    "        words = __normalize_words(words)\n",
    "    return words\n",
    "\n",
    "def filter_title(title: str) -> str:\n",
    "    titles = normalize_words(title, unique=False)\n",
    "    return ','.join([t for t in titles if t in include_words])\n",
    "\n",
    "train_df = cross_val(CFG)\n",
    "cpc_codes = pd.read_csv(\"./dataset/titles.csv\", engine='python')\n",
    "\n",
    "norm_titles = normalize_words(cpc_codes['title'].to_numpy(), unique=False) # ì—¬ê¸°ëŠ” big query datasetì„ ì •ê·œí™”\n",
    "anchor_targets = train_df['target'].unique().tolist() + train_df['anchor'].unique().tolist() # original train dataset \n",
    "norm_anchor_targets = normalize_words(anchor_targets) # Original Train Dataset ì •ê·œí™”\n",
    "include_words = set(norm_titles) & norm_anchor_targets # Anchor & Target ê³µí†µë˜ëŠ” ë‹¨ì–´ \n",
    "\n",
    "# ì™œ ì½”ë“œ ê¸¸ì´ê°€ 4ê°œ ì´ìƒì¸ ê²ƒë§Œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì¸ì§€... ì›ë³¸ ë°ì´í„°ëŠ” sub-classê¹Œì§€ ì œê³µ ì•ˆí•˜ëŠ” ê±¸ë¡œ ì•„ëŠ”ë°\n",
    "tmp_cpc_codes = cpc_codes.copy()\n",
    "tmp_cpc_codes = tmp_cpc_codes[cpc_codes['code'].str.len() >= 4] \n",
    "\n",
    "\"\"\"\n",
    "1) ì›ë³¸ ë°ì´í„°ì²˜ëŸ¼ í˜•íƒœë¥¼ ë§žì¶°ì£¼ë ¤ê³  3ìžë¦¬ê¹Œì§€ ìŠ¬ë¼ì´ì‹±í•´ì„œ ìƒˆë¡œìš´ ì»¬ëŸ¼ì„ ë§Œë“ ë‹¤.\n",
    "2) section_class 1: ['sub_class 1', 'sub_class 2', 'sub_class 3', .....]\n",
    "    - sub_classì˜ sub_class textê¹Œì§€ ëª¨ë‘ í¬í•¨\n",
    "3) ë°ì´í„° ë³€í™˜ ê²€ì¦\n",
    "4) ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ë­‰ê°œê³ , ì¤„ê¸€ë¡œ ë³€í™˜ & ì •ê·œí™”\n",
    "5) Anchor & Target êµì§‘í•© ë°ì´í„° ë”°ë¡œ ëª¨ì•„ ìƒˆë¡œìš´ ì—´ì— ì‚½ìž…\n",
    "    - ì´í›„ CountVectorizer ì ìš©í•˜ê¸° ìœ„í•¨ì¸ë“¯\n",
    "\"\"\"\n",
    "tmp_cpc_codes['section_class'] = tmp_cpc_codes['code'].apply(lambda x: x[:3]) \n",
    "title_group_df = tmp_cpc_codes.groupby('section_class', as_index=False)[['title']].agg(list)\n",
    "title_group_df = title_group_df[title_group_df['section_class'].str.len() == 3] \n",
    "title_group_df['title'] = title_group_df['title'].apply(lambda lst: ' '.join(lst))\n",
    "title_group_df['norm_title'] = title_group_df['title'].agg(filter_title)\n",
    "\n",
    "\"\"\"\n",
    "1) CountVectorizer ì ìš©\n",
    " - ê°œë³„ ë‹¨ì–´ì˜ ì¶œí˜„ë¹ˆë„ ê³„ì‚°\n",
    "2) ë²¡í„°ë¥¼ ë‹¤ì‹œ ì›ë³¸ ë‹¨ì–´ë¡œ ë³€í™˜\n",
    "\"\"\"\n",
    "vectorizer = CountVectorizer()\n",
    "c_vect = vectorizer.fit_transform(title_group_df['norm_title'])\n",
    "r = np.argsort(c_vect.toarray(), axis=1)[:, ::-1][::, :400]\n",
    "vect_words = vectorizer.get_feature_names_out()\n",
    "t_words = np.vectorize(lambda v: vect_words[v])(r)\n",
    "norm_title = title_group_df['norm_title'].str.split(',').to_numpy().tolist()\n",
    "\n",
    "\"\"\"\n",
    "1) ì´í•´ê°€ ì•ˆê°\n",
    "2) class ì •ë³´ ì¶”ê°€\n",
    "    - sub_class ì œëª© ì¶”ê°€\n",
    "3) context_text: main class + [SEP] + sub class\n",
    "4) cpc_text => {'A01': ('A01's title [SEP] A01B's title.........)}\n",
    "\"\"\"\n",
    "res = []\n",
    "for (n, t) in zip(norm_title, t_words):\n",
    "    res.append(','.join(set(n) & set(t)))\n",
    "title_group_df['norm_title'] = res\n",
    "title_group_df['section'] = title_group_df.section_class.str[0:1]\n",
    "title_group_df['section_title'] = title_group_df['section'].map(cpc_codes.set_index('code')['title']).str.lower() + ';' + title_group_df['section_class'].map(cpc_codes.set_index('code')['title']).str.lower()\n",
    "title_group_df['context_text'] = title_group_df['section_title'] + ' [SEP] ' + title_group_df['norm_title']\n",
    "cpc_texts = dict(title_group_df[['section_class', 'context_text']].to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"[Merge Two DataFrame]\"\"\"\n",
    "af_dict = {}\n",
    "for i,r in train_df[['anchor', 'fold']].iterrows():\n",
    "    af_dict[r.anchor] = r.fold\n",
    "anchor_context_grouped_target = train_df.groupby(['anchor', 'context'])['target'].apply(list)\n",
    "anchor_context_grouped_score = train_df.groupby(['anchor', 'context'])['score'].apply(list)\n",
    "anchor_context_grouped_id = train_df.groupby(['anchor', 'context'])['id'].apply(list)\n",
    "i = pd.DataFrame(anchor_context_grouped_id).reset_index()\n",
    "s = pd.DataFrame(anchor_context_grouped_score).reset_index()\n",
    "t = pd.DataFrame(anchor_context_grouped_target).reset_index()\n",
    "train_df = s.merge(t, on=['anchor', 'context'])\n",
    "train_df = train_df.merge(i, on=['anchor', 'context'])\n",
    "train_df['context_text'] = train_df['context'].map(cpc_texts)\n",
    "train_df = train_df.rename(columns={'target': 'targets', 'score': 'scores', 'id': 'ids'})\n",
    "train_df['fold'] = train_df['anchor'].map(af_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./output/tokenizer/tokenizer_config.json',\n",
       " './output/tokenizer/special_tokens_map.json',\n",
       " './output/tokenizer/spm.model',\n",
       " './output/tokenizer/added_tokens.json',\n",
       " './output/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Special Token\n",
    "tar_token = '[TAR]'\n",
    "special_tokens_dict = {'additional_special_tokens': [f'{tar_token}']}\n",
    "CFG.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tar_token_id = CFG.tokenizer(f'{tar_token}', add_special_tokens=False)['input_ids'][0]\n",
    "# logger.info(f'tar_token_id: {tar_token_id}')\n",
    "setattr(CFG.tokenizer, 'tar_token', f'{tar_token}')\n",
    "setattr(CFG.tokenizer, 'tar_token_id', tar_token_id)\n",
    "CFG.tokenizer.save_pretrained(f'{CFG.output_dir}tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of Train Data (anchor + targets + context_text)\n",
    "# token_test = train_df.explode('targets')\n",
    "\n",
    "# anchor_list = token_test.anchor.to_list()\n",
    "# targets_list = token_test.targets.to_list()\n",
    "# context_list = token_test.context_text.to_list()\n",
    "\n",
    "# text_len, text_token = [], []\n",
    "# text_list = [anchor_list[idx] + targets_list[idx] + context_list[idx] for idx in range(len(anchor_list))]\n",
    "# for text in tqdm(text_list):\n",
    "#     inputs = CFG.tokenizer.encode_plus(\n",
    "#         text,\n",
    "#         return_tensors = None,\n",
    "#         add_special_tokens = False,\n",
    "#         truncation = False,\n",
    "# #        max_length = 192,\n",
    "#         return_token_type_ids = True,\n",
    "#         return_attention_mask = True    \n",
    "#     )\n",
    "#     text_token.append(CFG.tokenizer.decode(inputs.input_ids))\n",
    "#     text_len.append(len(inputs.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘©â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦ Step 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UPPPMDataset(Dataset):\n",
    "    def __init__(self, CFG, df, is_valid=False):\n",
    "        super().__init__()\n",
    "        self.anchor_list = df.anchor.to_numpy()\n",
    "        self.target_list = df.targets.to_numpy()\n",
    "        self.context_list = df.context_text.to_numpy()\n",
    "        self.score_list = df.scores.to_numpy()\n",
    "        self.id_list = df.ids.to_numpy()\n",
    "        self.cfg = CFG\n",
    "        self.is_valid = is_valid\n",
    "        \n",
    "#     def add_special_token(self):\n",
    "#         tar_token = '[TAR]'\n",
    "#         special_tokens_dict = {'additional_special_tokens': [f'{tar_token}']}\n",
    "#         self.cfg.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "#         tar_token_id = self.cfg.tokenizer(f'{tar_token}', add_special_tokens=False)['input_ids'][0]\n",
    "#         # logger.info(f'tar_token_id: {tar_token_id}')\n",
    "#         setattr(self.cfg.tokenizer, 'tar_token', f'{tar_token}')\n",
    "#         setattr(self.cfg.tokenizer, 'tar_token_id', tar_token_id)\n",
    "#         self.cfg.tokenizer.save_pretrained(f'{self.cfg.output_dir}tokenizer/')\n",
    "    \n",
    "    def tokenizing(self, text_data: str) -> dict:\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            text_data, \n",
    "            return_tensors=None, # if true, tf.tensor, pt.tensor, numpy\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.cfg.max_len\n",
    "        )\n",
    "        return inputs\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.id_list)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> \"tuple[dict, Tensor, Tensor]\":\n",
    "        \"\"\"\n",
    "        1) make Embedding Shape,\n",
    "            - Data: [cls]+[anchor]+[sep]+[target]+[tar]+[target]+[tar]...+[tar]+[cpc_text]+[sep]\n",
    "            - Label: [-1] * self.cfg.max_len, target valueì˜ ì¸ë±ìŠ¤ ìœ„ì¹˜ì— score_classê°’ ì „ë‹¬\n",
    "        2) apply data augment\n",
    "            - shuffle target values\n",
    "        \"\"\"\n",
    "        scores = np.array(self.score_list[idx]) # len(scores) == target count\n",
    "        target_mask = np.zeros(self.cfg.max_len)\n",
    "        targets = np.array(self.target_list[idx])\n",
    "        \n",
    "        # Data Augment for train stage\n",
    "        if not self.is_valid:\n",
    "            indices = list(range(len(scores)))\n",
    "            random.shuffle(indices)\n",
    "            scores = scores[indices]\n",
    "            targets = targets[indices]\n",
    "        \n",
    "        text = self.cfg.tokenizer.cls_token + self.anchor_list[idx] + self.cfg.tokenizer.sep_token\n",
    "        for target in targets:\n",
    "            text += target + self.cfg.tokenizer.tar_token\n",
    "        text += self.context_list[idx] + self.cfg.tokenizer.sep_token\n",
    "        \n",
    "        # tokenizing & make label list\n",
    "        inputs = self.tokenizing(text) \n",
    "        label = torch.full([self.cfg.max_len], -1, dtype=torch.float)\n",
    "        # target valueì˜ ì¸ë±ìŠ¤(label list) ìœ„ì¹˜ì— scoreê°’ ìžì²´ë¥¼ ì „ë‹¬\n",
    "        # ë‚˜ì¤‘ì— score_classë¡œ ë³€í™˜í•´ì£¼ëŠ” ìž‘ì—…ì´ í•„ìš”í•  ë“¯\n",
    "        cnt_tar = 0\n",
    "        cnt_sep = 0\n",
    "        nth_target = -1\n",
    "        prev_i = -1\n",
    "        \n",
    "        for i, input_id in enumerate(inputs['input_ids']):\n",
    "            if input_id == self.cfg.tokenizer.tar_token_id:\n",
    "                cnt_tar += 1\n",
    "                if cnt_tar == len(targets):\n",
    "                    break\n",
    "            if input_id == self.cfg.tokenizer.sep_token_id:\n",
    "                cnt_sep += 1\n",
    "            \n",
    "            if cnt_sep == 1 and input_id not in [self.cfg.tokenizer.pad_token_id, self.cfg.tokenizer.sep_token_id, self.cfg.tokenizer.tar_token_id]:\n",
    "                if (i-prev_i) > 1:\n",
    "                    nth_target += 1\n",
    "                label[i] = scores[nth_target]\n",
    "                target_mask[i] = 1\n",
    "                prev_i = i\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "            \n",
    "        return inputs, target_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 4. Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UPPPMModel(nn.Module):\n",
    "    def __init__(self, CFG, n_vocabs: int):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG\n",
    "        self.auto_cfg = AutoConfig.from_pretrained(CFG.model_name,\n",
    "                                                   output_hidden_states = True)\n",
    "        self.model = AutoModel.from_pretrained(CFG.model_name,\n",
    "                                               config = self.auto_cfg)\n",
    "        self.model.resize_token_embeddings(n_vocabs)\n",
    "        \"\"\"\n",
    "        1) if your loss function == CrossEntropyLoss, change value 1 to 5\n",
    "        2) fully_connected == classifier        \n",
    "        \"\"\"\n",
    "        self.fc = nn.Linear(self.auto_cfg.hidden_size, 1)\n",
    "        self._init_weights(self.fc) # Classifier Layer Init\n",
    "        # checkpointing\n",
    "        if self.cfg.gradient_checkpoint:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        # re-init Top-K Encoder Layer\n",
    "        if self.cfg.reinit:\n",
    "            self.reinit_topk_layers()\n",
    "    \n",
    "    # Classifier Layer Init\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    # Re-Init Top-K Transformers Layer\n",
    "    def reinit_topk_layers(self):\n",
    "        \"\"\"\n",
    "        Re-initialize the last-k transformer layers.\n",
    "        Args:\n",
    "            model: The target transformer model.\n",
    "            num_layers: The number of layers to be re-initialized.\n",
    "        \"\"\"\n",
    "        self.model.encoder.layer[-self.cfg.num_reinit:].apply(self.model._init_weights) # model classì— ìžˆëŠ”ê±°\n",
    "        \n",
    "    def forward(self, inputs: dict):\n",
    "        \"\"\"\n",
    "        outputs.last_hidden_stats with no pooling => Token-Level Task\n",
    "        \"\"\"\n",
    "        outputs = self.model(**inputs) # inputs from LECRDataset\n",
    "        embedding = outputs.last_hidden_state\n",
    "        output = self.fc(embedding).squeeze(-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤ Step 5. Model & Metric Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adversarial Weight Perturbation\n",
    "\"\"\"\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        awp: bool,\n",
    "        adv_param: str=\"weight\",\n",
    "        adv_lr: float=1.0,\n",
    "        adv_eps: float=0.01\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.awp = awp\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "    def attack_backward(self, inputs: dict, label):\n",
    "        with torch.cuda.amp.autocast(enabled=self.awp):\n",
    "            self._save()\n",
    "            self._attack_step()\n",
    "            y_preds = self.model(inputs)\n",
    "            adv_loss = self.criterion(\n",
    "                y_preds.view(-1, 1), label.view(-1, 1))\n",
    "            mask = (label.view(-1, 1) != -1)\n",
    "            adv_loss = torch.masked_select(adv_loss, mask).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "        return adv_loss\n",
    "\n",
    "    def _attack_step(self) -> None:\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(\n",
    "                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "\n",
    "    def _save(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count        \n",
    "        \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps # If MSE == 0, We need eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "    \n",
    "def collate(inputs: dict) -> dict:\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "\n",
    "# Pearson Correlations Coeffiicient Loss\n",
    "class PearsonLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true) -> float:\n",
    "        x = y_pred.clone()\n",
    "        y = y_true.clone()\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "        cov = torch.sum(vx * vy)\n",
    "        corr = cov / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + 1e-12)\n",
    "        corr = torch.maximum(torch.minimum(corr,torch.tensor(1)), torch.tensor(-1))\n",
    "        \n",
    "        return torch.sub(torch.tensor(1), corr ** 2)\n",
    "    \n",
    "\n",
    "# F2-Score for each fold\n",
    "# Multi Classification => Positive Case would be highlighted, So We need to use Recall to Eval Metric\n",
    "\"\"\"\n",
    "ì˜¤ì°¨í–‰ë ¬ ë° ê´€ë ¨ ì§€í‘œ ì •ë¦¬\n",
    "1) ëª¨ë“ ê±´ í•­ìƒ ì˜ˆì¸¡ê°’ ê¸°ì¤€ìœ¼ë¡œ ìƒê°\n",
    "    - ì˜ˆì¸¡ê°’ì´ Positive && ì‹¤ì œ ì •ë‹µì¸ ê²½ìš° => True Positive, TP\n",
    "    - ì˜ˆì¸¡ê°’ì´ Positive && ì‹¤ì œ ì˜¤ë‹µì¸ ê²½ìš° => False Positive, FP\n",
    "    - ì˜ˆì¸¡ê°’ì´ Negative && ì‹¤ì œ ì˜¤ë‹µì¸ ê²½ìš° => True Negative, TN\n",
    "    - ì˜ˆì¸¡ê°’ì´ Negative && ì‹¤ì œ ì •ë‹µì¸ ê²½ìš° ==> False Negative, FN\n",
    "\n",
    "2) Precision\n",
    "    - precision = TP / (TP + FP)\n",
    "      => ì •ë‹µìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ì—ì„œ ì‹¤ì œ ì •ë‹µì— í•´ë‹¹ í•˜ëŠ” ê²½ìš°\n",
    "    - Recall = TP / (TP + FN)\n",
    "      => ì‹¤ì œ ì •ë‹µ ì¤‘ì—ì„œ ëª¨ë¸ì´ ì˜ˆì¸¡ ì„±ê³µí•œ ê²ƒì´ ëª‡ê°œ ì¸ê°€\n",
    "\"\"\"\n",
    "# def pearson_score(y_true, y_pred) -> float:\n",
    "#     return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "def pearson_score(y_true, y_pred) -> float:\n",
    "    x, y = y_pred, y_true\n",
    "    vx = x - np.mean(x)\n",
    "    vy = y - np.mean(y)\n",
    "    cov = np.sum(vx * vy)\n",
    "    corr = cov / (np.sqrt(np.sum(vx ** 2)) * np.sqrt(np.sum(vy ** 2)) + 1e-12)\n",
    "    return corr\n",
    "    \n",
    "def recall(y_true, y_pred) -> float:\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    recall = tp / (tp + fn)\n",
    "    return round(recall.mean(), 4)\n",
    "\n",
    "def precision(y_true, y_pred) -> float:\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    return round(precision.mean(), 4)\n",
    "\n",
    "def f2_score(y_true, y_pred) -> float:\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2.8 Trainer Input Class\n",
    "# Later append for AWP, SWA, Re-Init Code\n",
    "class TrainInput():\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "        self.df = df # return dataset\n",
    "        if self.cfg.gradient_checkpoint:\n",
    "            self.save_parameter = f'(best_score) {self.cfg.model_name}_state_dict.pth' # checkpoint\n",
    "        \n",
    "    \n",
    "    # LLRD \n",
    "    def get_optimizer_grouped_parameters(self, model, layerwise_lr, layerwise_weight_decay, layerwise_lr_decay):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        # initialize lr for task specific layer\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "                                         \"weight_decay\": 0.0,\n",
    "                                         \"lr\": layerwise_lr,\n",
    "                                        },]\n",
    "        # initialize lrs for every layer\n",
    "        layers = [model.model.embeddings] + list(model.model.encoder.layer)\n",
    "        layers.reverse()\n",
    "        lr = layerwise_lr\n",
    "        for layer in layers:\n",
    "            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": layerwise_weight_decay,\n",
    "                                              \"lr\": lr,\n",
    "                                             },\n",
    "                                             {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": 0.0,\n",
    "                                              \"lr\": lr,\n",
    "                                             },]\n",
    "            lr *= layerwise_lr_decay\n",
    "        return optimizer_grouped_parameters\n",
    "    \n",
    "    def get_optimizer_params(self, model, encoder_lr, decoder_lr, weight_decay):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "\n",
    "    def make_batch(self, fold: int):\n",
    "        train = self.df[self.df['fold'] != fold].reset_index(drop=True)\n",
    "        valid = self.df[self.df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Custom Dataset\n",
    "        train_dataset = UPPPMDataset(self.cfg, train)\n",
    "        valid_dataset = UPPPMDataset(self.cfg, valid, is_valid=True)\n",
    "        valid_labels = valid['scores'].explode().to_numpy()\n",
    "\n",
    "        # DataLoader\n",
    "        loader_train = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size = self.cfg.batch_size,\n",
    "            shuffle = True,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "            num_workers = self.cfg.num_workers,\n",
    "            pin_memory = True,\n",
    "            drop_last = True,\n",
    "        )\n",
    "        \n",
    "        loader_valid = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size = self.cfg.batch_size,\n",
    "            shuffle = False,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "            num_workers = self.cfg.num_workers,\n",
    "            pin_memory = True,\n",
    "            drop_last = False,\n",
    "        )\n",
    "        \n",
    "        return loader_train, loader_valid, train, valid, valid_labels\n",
    "\n",
    "    def model_setting(self):\n",
    "        \"\"\"\n",
    "        [model]\n",
    "        1) Re-Initialze Weights of Encoder\n",
    "           - DeBERTa => Last Two Layers == EMD        \n",
    "        2) SWA\n",
    "           - original model => to.device\n",
    "           - after calculate, update swa_model\n",
    "        \"\"\"\n",
    "        model = UPPPMModel(self.cfg, n_vocabs=len(self.tokenizer))        \n",
    "        #model.load_state_dict(torch.load('Token_Classification_Fold0_DeBERTa_V3_Large.pth'))\n",
    "        model.to(self.cfg.device)\n",
    "        # SWA: Stochastic Weighted Averaging        \n",
    "        if self.cfg.swa:\n",
    "            swa_model = AveragedModel(model)\n",
    "        else:\n",
    "            swa_model = 'none'\n",
    "        \n",
    "        # Setting Loss_Function\n",
    "        # Because we don't need to calculate none-target token: Output will be same shape as input\n",
    "        if self.cfg.loss_fn == 'BCE':\n",
    "            criterion = nn.BCEWithLogitsLoss(reduction='none') \n",
    "        if self.cfg.loss_fn == 'cross_entropy':\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        if self.cfg.loss_fn == 'pearson':\n",
    "            criterion = PearsonLoss()\n",
    "        if self.cfg.loss_fn == 'RMSE':\n",
    "            criterion = RMSELoss()\n",
    "            \n",
    "        # optimizer\n",
    "        grouped_optimizer_params = self.get_optimizer_grouped_parameters(\n",
    "            model, \n",
    "            self.cfg.layerwise_lr, \n",
    "            self.cfg.layerwise_weight_decay, \n",
    "            self.cfg.layerwise_lr_decay\n",
    "        )\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            grouped_optimizer_params,\n",
    "            lr = self.cfg.layerwise_lr,\n",
    "            eps = self.cfg.layerwise_adam_epsilon,\n",
    "            correct_bias = not self.cfg.layerwise_use_bertadam)\n",
    "        \n",
    "        return model, swa_model, criterion, optimizer, self.save_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3.1 Train & Validation Function\n",
    "def train_fn(cfg,\n",
    "             loader_train,\n",
    "             loader_valid,\n",
    "             model,\n",
    "             criterion,\n",
    "             optimizer,\n",
    "             scheduler,\n",
    "             valid,\n",
    "             valid_labels,\n",
    "             epoch,\n",
    "             swa_model = None,\n",
    "             swa_start = None,\n",
    "             swa_scheduler = None,):\n",
    "    # Train Stages\n",
    "    # torch.amp.gradscaler\n",
    "    awp = AWP(model, criterion, optimizer, cfg.awp, adv_lr=cfg.awp_lr, adv_eps=cfg.awp_eps)\n",
    "    if cfg.amp_scaler:\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    global_step, score_list = 0, [] # All Fold's average of mean F2-Score\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    for step, (inputs, _, labels) in enumerate(tqdm(loader_train)):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(cfg.device) # train to gpu\n",
    "        labels = labels.to(cfg.device) # label to gpu\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = cfg.amp_scaler):\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds.view(-1, 1), labels.view(-1,1))\n",
    "            mask = (labels.view(-1, 1) != -1)\n",
    "            loss = torch.masked_select(loss, mask).mean() # reduction = mean\n",
    "            losses.update(loss, batch_size)\n",
    "            \"\"\"\n",
    "            [gradient_accumlation]\n",
    "            - GPU VRAM OVER ë¬¸ì œí•´ê²°ì„ ìœ„í•´ ì‚¬ìš©\n",
    "            - epochì´ ì‚¬ìš©ìž ì§€ì • ì—í­ íšŸìˆ˜ë¥¼ ë„˜ì„ ë•Œê¹Œì§€ Backward í•˜ì§€ ì•Šê³  ê·¸ë¼ë””ì–¸íŠ¸ ì¶•ì \n",
    "            - ì§€ì • epoch ë„˜ì–´ê°€ë©´ í•œ ë²ˆì— Backward\n",
    "            \"\"\"\n",
    "            if cfg.n_gradient_accumulation_steps > 1:\n",
    "                loss = loss / cfg.n_gradient_accumulation_steps\n",
    "                \n",
    "        scaler.scale(loss).backward()        \n",
    "        \"\"\"\n",
    "        [Adversarial Weight Training]\n",
    "        \"\"\"\n",
    "        if cfg.awp and epoch >= cfg.nth_awp_start_epoch:\n",
    "            loss = awp.attack_backward(inputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            awp._restore()\n",
    "        \"\"\"\n",
    "        1) Clipping Gradient && Gradient Accumlation\n",
    "        2) Stochastic Weight Averaging\n",
    "        \"\"\"\n",
    "        if cfg.clipping_grad and ((step + 1) % cfg.n_gradient_accumulation_steps == 0 or cfg.n_gradient_accumulation_steps == 1):\n",
    "            scaler.unscale_(optimizer)      \n",
    "            grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "                model.parameters(),\n",
    "                cfg.max_grad_norm\n",
    "            )\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if epoch >= int(swa_start):\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step()  \n",
    "            global_step += 1\n",
    "            scheduler.step()\n",
    "            \n",
    "    # Validation Stage\n",
    "    preds_list, label_list = [], []\n",
    "    valid_losses = AverageMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (inputs, target_masks, labels) in enumerate(tqdm(loader_valid)):\n",
    "            inputs = collate(inputs)\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(cfg.device)\n",
    "            labels = labels.to(cfg.device)\n",
    "            batch_size = labels.size(0)\n",
    "            preds = model(inputs)\n",
    "            valid_loss = criterion(preds.view(-1, 1), labels.view(-1, 1))\n",
    "            mask = (labels.view(-1, 1) != -1)\n",
    "            valid_loss = torch.masked_select(valid_loss, mask).mean()\n",
    "            valid_losses.update(valid_loss, batch_size)\n",
    "\n",
    "            y_preds = preds.sigmoid().to('cpu').numpy()\n",
    "                                          \n",
    "            anchorwise_preds = []\n",
    "            for pred, target_mask, in zip(y_preds, target_masks):\n",
    "                prev_i = -1\n",
    "                targetwise_pred_scores = []\n",
    "                for i, (p, tm) in enumerate(zip(pred, target_mask)):\n",
    "                    if tm != 0:\n",
    "                        if i-1 == prev_i:\n",
    "                            targetwise_pred_scores[-1].append(p)\n",
    "                        else:\n",
    "                            targetwise_pred_scores.append([p])\n",
    "                        prev_i = i\n",
    "                for targetwise_pred_score in targetwise_pred_scores:\n",
    "                    anchorwise_preds.append(np.mean(targetwise_pred_score))\n",
    "            preds_list.append(anchorwise_preds)\n",
    "    # error_list = [[i, preds_list.index(i)] for i in preds_list if i == 'nan' or i == float('inf')]\n",
    "    # print(error_list)\n",
    "    epoch_score = pearson_score(valid_labels, np.array(reduce(lambda a, b: a + b, preds_list)))\n",
    "    return losses.avg, valid_losses.avg, epoch_score, grad_norm, scheduler.get_lr()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swa_valid(cfg,\n",
    "              loader_valid,\n",
    "              swa_model,\n",
    "              criterion,\n",
    "              valid_labels):\n",
    "    swa_preds_list, swa_label_list = [], []\n",
    "    swa_model.eval()\n",
    "    swa_valid_losses = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (swa_inputs, target_masks, swa_labels) in enumerate(tqdm(loader_valid)):\n",
    "            swa_inputs = collate(swa_inputs)\n",
    "            \n",
    "            for k, v in swa_inputs.items():\n",
    "                swa_inputs[k] = v.to(cfg.device)\n",
    "                \n",
    "            swa_labels = swa_labels.to(cfg.device)\n",
    "            batch_size = swa_labels.size(0)\n",
    "            \n",
    "            swa_preds = swa_model(swa_inputs)\n",
    "            \n",
    "            swa_valid_loss = criterion(swa_preds.view(-1, 1), swa_labels.view(-1, 1))\n",
    "            mask = (swa_labels.view(-1, 1) != -1)\n",
    "            swa_valid_loss = torch.masked_select(swa_valid_loss, mask)\n",
    "            swa_valid_loss = swa_valid_loss.mean()\n",
    "            swa_valid_losses.update(swa_valid_loss, batch_size)\n",
    "            \n",
    "            swa_y_preds = swa_preds.sigmoid().to('cpu').numpy()\n",
    "                                          \n",
    "            anchorwise_preds = []\n",
    "            for pred, target_mask, in zip(swa_y_preds, target_masks):\n",
    "                prev_i = -1\n",
    "                targetwise_pred_scores = []\n",
    "                for i, (p, tm) in enumerate(zip(pred, target_mask)):\n",
    "                    if tm != 0:\n",
    "                        if i-1 == prev_i:\n",
    "                            targetwise_pred_scores[-1].append(p)\n",
    "                        else:\n",
    "                            targetwise_pred_scores.append([p])\n",
    "                        prev_i = i\n",
    "                for targetwise_pred_score in targetwise_pred_scores:\n",
    "                    anchorwise_preds.append(np.mean(targetwise_pred_score))\n",
    "                    \n",
    "            swa_preds_list.append(anchorwise_preds)\n",
    "    swa_valid_score = pearson_score(valid_labels, np.array(reduce(lambda a, b: a + b, swa_preds_list)))    \n",
    "    del swa_preds_list, swa_y_preds, swa_labels, anchorwise_preds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return swa_valid_losses.avg, swa_valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqcqced\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ce4a4d223c4dbcba1d721a76f4b03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666878751666445, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qcqced/ë°”íƒ•í™”ë©´/ML_Test/UPPPM/wandb/run-20230331_231942-exddeib5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification/runs/exddeib5' target=\"_blank\">[Append Version 2.2]Fold 0microsoft/deberta-v3-large</a></strong> to <a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification' target=\"_blank\">https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification/runs/exddeib5' target=\"_blank\">https://wandb.ai/qcqced/%5BAppend%20Ver%202%5DUPPPM%20Token%20Classification/runs/exddeib5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Retriever Model :microsoft/deberta-v3-large =========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3d4fcdd09f4b3b92c7bd90eb95a90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 1th Fold Train & Validation ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762cd2bb0d744a1fba4c5198b442ba74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9513a0ade04bd1bd5d7981af8aa445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/180] Train Loss: 0.6843000054359436\n",
      "[1/180] Valid Loss: 0.6578999757766724\n",
      "[1/180] Pearson Score: 0.4097\n",
      "[1/180] Gradient Norm: 1780.4930419921875\n",
      "[1/180] lr: 2.7122321670735012e-06\n",
      "[Update] Valid Score : (-inf => 0.4097) Save Parameter\n",
      "Best Score: 0.40971906912197553\n",
      "[2/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537bb92b0795468da3adb350dcdfcf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f2339b40ff4851ac0d7d42f8174cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/180] Train Loss: 0.6531999707221985\n",
      "[2/180] Valid Loss: 0.6401000022888184\n",
      "[2/180] Pearson Score: 0.532\n",
      "[2/180] Gradient Norm: 2407.181640625\n",
      "[2/180] lr: 5.4244643341470025e-06\n",
      "[Update] Valid Score : (0.4097 => 0.5320) Save Parameter\n",
      "Best Score: 0.5319613917782015\n",
      "[3/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae469b0ac264239b9fabae62664c2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e59c5a8b2045728838fc1fee255847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/180] Train Loss: 0.640500009059906\n",
      "[3/180] Valid Loss: 0.6269999742507935\n",
      "[3/180] Pearson Score: 0.6087\n",
      "[3/180] Gradient Norm: 3722.11279296875\n",
      "[3/180] lr: 8.136696501220503e-06\n",
      "[Update] Valid Score : (0.5320 => 0.6087) Save Parameter\n",
      "Best Score: 0.608674825045642\n",
      "[4/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d2db444478420ca103db62ebf49a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca7087abeb6411b8fa03c822407e0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/180] Train Loss: 0.6287000179290771\n",
      "[4/180] Valid Loss: 0.6136999726295471\n",
      "[4/180] Pearson Score: 0.6612\n",
      "[4/180] Gradient Norm: 8064.52001953125\n",
      "[4/180] lr: 1.0848928668294005e-05\n",
      "[Update] Valid Score : (0.6087 => 0.6612) Save Parameter\n",
      "Best Score: 0.661183444323874\n",
      "[5/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25537e4d8b664624ab9640173f4973dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18192985e2444b96bdccd5f3143b5e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/180] Train Loss: 0.6150000095367432\n",
      "[5/180] Valid Loss: 0.5963000059127808\n",
      "[5/180] Pearson Score: 0.6958\n",
      "[5/180] Gradient Norm: 8425.140625\n",
      "[5/180] lr: 1.3561160835367507e-05\n",
      "[Update] Valid Score : (0.6612 => 0.6958) Save Parameter\n",
      "Best Score: 0.6957966041860348\n",
      "[6/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d72ef8259d4e4193fe97843922e474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c38f092ee44e26b6282bd08957d31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/180] Train Loss: 0.5999000072479248\n",
      "[6/180] Valid Loss: 0.5839999914169312\n",
      "[6/180] Pearson Score: 0.7249\n",
      "[6/180] Gradient Norm: 4018.826904296875\n",
      "[6/180] lr: 1.6273393002441007e-05\n",
      "[Update] Valid Score : (0.6958 => 0.7249) Save Parameter\n",
      "Best Score: 0.724850180184517\n",
      "[7/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ae85c110a14ccd865a397e41a96e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659aa570cb964c16bfcea42aef1151dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/180] Train Loss: 0.5882999897003174\n",
      "[7/180] Valid Loss: 0.5703999996185303\n",
      "[7/180] Pearson Score: 0.7546\n",
      "[7/180] Gradient Norm: 5371.4365234375\n",
      "[7/180] lr: 1.898562516951451e-05\n",
      "[Update] Valid Score : (0.7249 => 0.7546) Save Parameter\n",
      "Best Score: 0.7545719150912469\n",
      "[8/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e9a8e3ed444fd9a7fae7b9f10e2a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d3122bed1d43ce9524fdf9befe56ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/180] Train Loss: 0.5796999931335449\n",
      "[8/180] Valid Loss: 0.5673999786376953\n",
      "[8/180] Pearson Score: 0.7706\n",
      "[8/180] Gradient Norm: 6399.50830078125\n",
      "[8/180] lr: 2.169785733658801e-05\n",
      "[Update] Valid Score : (0.7546 => 0.7706) Save Parameter\n",
      "Best Score: 0.7706341882087255\n",
      "[9/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d9e46e8b0a4954ae1be0df919c6b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2561aa006cc0403798acf13151cb7bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/180] Train Loss: 0.5719000101089478\n",
      "[9/180] Valid Loss: 0.559499979019165\n",
      "[9/180] Pearson Score: 0.7849\n",
      "[9/180] Gradient Norm: 6250.1083984375\n",
      "[9/180] lr: 2.4410089503661513e-05\n",
      "[Update] Valid Score : (0.7706 => 0.7849) Save Parameter\n",
      "Best Score: 0.7848926436915169\n",
      "[10/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d56ecc7bef4679bdf15f526db1bb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f1c2f13e57452f80363bc4e1ae3e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/180] Train Loss: 0.5637999773025513\n",
      "[10/180] Valid Loss: 0.555899977684021\n",
      "[10/180] Pearson Score: 0.7985\n",
      "[10/180] Gradient Norm: 6449.2470703125\n",
      "[10/180] lr: 2.7122321670735013e-05\n",
      "[Update] Valid Score : (0.7849 => 0.7985) Save Parameter\n",
      "Best Score: 0.7985043920576239\n",
      "[11/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b6ef05f5094efc8899324079761bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d73482159a4c94af6bfc3cf046694b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/180] Train Loss: 0.5575000047683716\n",
      "[11/180] Valid Loss: 0.5504000186920166\n",
      "[11/180] Pearson Score: 0.8068\n",
      "[11/180] Gradient Norm: 5355.22265625\n",
      "[11/180] lr: 2.9834553837808517e-05\n",
      "[Update] Valid Score : (0.7985 => 0.8068) Save Parameter\n",
      "Best Score: 0.8067804918261992\n",
      "[12/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb60ab167a14aa28200ed7bc74e4769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e69f7b5a8e148d19e75a76d30b08f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/180] Train Loss: 0.5526000261306763\n",
      "[12/180] Valid Loss: 0.5532000064849854\n",
      "[12/180] Pearson Score: 0.813\n",
      "[12/180] Gradient Norm: 4369.27392578125\n",
      "[12/180] lr: 3.254678600488201e-05\n",
      "[Update] Valid Score : (0.8068 => 0.8130) Save Parameter\n",
      "Best Score: 0.8129845930798877\n",
      "[13/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7824fadb8a5e4df38a5444fc5991ac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc8e2cac6a74caba3a50aac915734a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/180] Train Loss: 0.5468999743461609\n",
      "[13/180] Valid Loss: 0.5467000007629395\n",
      "[13/180] Pearson Score: 0.8207\n",
      "[13/180] Gradient Norm: 3543.90869140625\n",
      "[13/180] lr: 3.525901817195551e-05\n",
      "[Update] Valid Score : (0.8130 => 0.8207) Save Parameter\n",
      "Best Score: 0.8207410904728175\n",
      "[14/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f04f01c4e834486af97c6afd7eabc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238e42d4fc8c4577bb390a68615239aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/180] Train Loss: 0.5428000092506409\n",
      "[14/180] Valid Loss: 0.5479000210762024\n",
      "[14/180] Pearson Score: 0.8222\n",
      "[14/180] Gradient Norm: 4250.6513671875\n",
      "[14/180] lr: 3.797125033902902e-05\n",
      "[Update] Valid Score : (0.8207 => 0.8222) Save Parameter\n",
      "Best Score: 0.8221954399992328\n",
      "[15/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62716cd09664705a749f26b68195c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63481501d30843ab8561b7405ee2ed72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/180] Train Loss: 0.5389000177383423\n",
      "[15/180] Valid Loss: 0.5475000143051147\n",
      "[15/180] Pearson Score: 0.8251\n",
      "[15/180] Gradient Norm: 2680.077392578125\n",
      "[15/180] lr: 4.068348250610252e-05\n",
      "[Update] Valid Score : (0.8222 => 0.8251) Save Parameter\n",
      "Best Score: 0.8250705062382038\n",
      "[16/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9dfc6aff4048a5ba45f2a1968a1c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e859159aa8e40d7a6d18ee1f5b1702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/180] Train Loss: 0.5353999733924866\n",
      "[16/180] Valid Loss: 0.5485000014305115\n",
      "[16/180] Pearson Score: 0.827\n",
      "[16/180] Gradient Norm: 4286.13134765625\n",
      "[16/180] lr: 4.339571467317602e-05\n",
      "[Update] Valid Score : (0.8251 => 0.8270) Save Parameter\n",
      "Best Score: 0.8269675036996958\n",
      "[17/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b8876908e64f6aa404351cf93acff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6102c52af6457182c4aee93157efb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/180] Train Loss: 0.5324000120162964\n",
      "[17/180] Valid Loss: 0.5525000095367432\n",
      "[17/180] Pearson Score: 0.8294\n",
      "[17/180] Gradient Norm: 6413.98828125\n",
      "[17/180] lr: 4.610794684024952e-05\n",
      "[Update] Valid Score : (0.8270 => 0.8294) Save Parameter\n",
      "Best Score: 0.8294301481083621\n",
      "[18/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acfb9abd2f4476c80ebef9fa2e6c365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168e4d41ad8a4681802e8cc0cc9ea813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/180] Train Loss: 0.5307999849319458\n",
      "[18/180] Valid Loss: 0.545799970626831\n",
      "[18/180] Pearson Score: 0.8343\n",
      "[18/180] Gradient Norm: 2480.564208984375\n",
      "[18/180] lr: 4.8820179007323027e-05\n",
      "[Update] Valid Score : (0.8294 => 0.8343) Save Parameter\n",
      "Best Score: 0.834254476997923\n",
      "[19/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23d37e13781482fb0b3f1ca028365e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dc69b5fa444bcbb54656c1a38d9c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/180] Train Loss: 0.5299000144004822\n",
      "[19/180] Valid Loss: 0.5479000210762024\n",
      "[19/180] Pearson Score: 0.8337\n",
      "[19/180] Gradient Norm: 1635.8603515625\n",
      "[19/180] lr: 4.9634645331736365e-05\n",
      "[20/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9df5584955441599b7b641f05f4185d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f95e95f141543f1a15b844c1d9b5eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/180] Train Loss: 0.5249999761581421\n",
      "[20/180] Valid Loss: 0.5479000210762024\n",
      "[20/180] Pearson Score: 0.8362\n",
      "[20/180] Gradient Norm: 3918.79931640625\n",
      "[20/180] lr: 4.724224290233164e-05\n",
      "[Update] Valid Score : (0.8343 => 0.8362) Save Parameter\n",
      "Best Score: 0.8361648769024356\n",
      "[21/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ee9884137e4fcd8e94b90d77a2558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4772093225449feb08253e6d392b74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/180] Train Loss: 0.5217000246047974\n",
      "[21/180] Valid Loss: 0.5519999861717224\n",
      "[21/180] Pearson Score: 0.8387\n",
      "[21/180] Gradient Norm: 2828.005859375\n",
      "[21/180] lr: 4.282391876428358e-05\n",
      "[Update] Valid Score : (0.8362 => 0.8387) Save Parameter\n",
      "Best Score: 0.8387472586881518\n",
      "[22/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2db6e4e51b43b4b47276c2847cfec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0f35c2264e458e8d5d596f93ad2117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/180] Train Loss: 0.5181999802589417\n",
      "[22/180] Valid Loss: 0.5485000014305115\n",
      "[22/180] Pearson Score: 0.8376\n",
      "[22/180] Gradient Norm: 3624.087890625\n",
      "[22/180] lr: 3.678211339207133e-05\n",
      "[23/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81a5f71307d40349be2e7c68fd34a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5c38b32eba4540989a2dbfdf223a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/180] Train Loss: 0.5163000226020813\n",
      "[23/180] Valid Loss: 0.5508000254631042\n",
      "[23/180] Pearson Score: 0.8412\n",
      "[23/180] Gradient Norm: 3202.439453125\n",
      "[23/180] lr: 2.9667141100519634e-05\n",
      "[Update] Valid Score : (0.8387 => 0.8412) Save Parameter\n",
      "Best Score: 0.8412145154352871\n",
      "[24/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99f32efff884104905854147074789f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ccde97ff7548fa959ee89083e9cf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/180] Train Loss: 0.5146999955177307\n",
      "[24/180] Valid Loss: 0.550000011920929\n",
      "[24/180] Pearson Score: 0.84\n",
      "[24/180] Gradient Norm: 3588.76953125\n",
      "[24/180] lr: 2.212706498673368e-05\n",
      "[25/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4ab6e4f4aa4b3b8d4cbb618869de76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc37616def6843b6b1e3b4f93c5fd248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/180] Train Loss: 0.51419997215271\n",
      "[25/180] Valid Loss: 0.555899977684021\n",
      "[25/180] Pearson Score: 0.8385\n",
      "[25/180] Gradient Norm: 5315.53369140625\n",
      "[25/180] lr: 1.48486684812703e-05\n",
      "[26/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089ea89e88ec4bf5b4a6cc11a41681e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4097936f2d74f3f94d3776262002f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/180] Train Loss: 0.5115000009536743\n",
      "[26/180] Valid Loss: 0.5515999794006348\n",
      "[26/180] Pearson Score: 0.8414\n",
      "[26/180] Gradient Norm: 4035.801513671875\n",
      "[26/180] lr: 8.494900080343715e-06\n",
      "[Update] Valid Score : (0.8412 => 0.8414) Save Parameter\n",
      "Best Score: 0.8414331603470655\n",
      "[27/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e3a6376c2b4dac9ab49ba7bac6cca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c1800dd2cb4a99872bfb9ea5b7d7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/180] Train Loss: 0.511900007724762\n",
      "[27/180] Valid Loss: 0.5548999905586243\n",
      "[27/180] Pearson Score: 0.8409\n",
      "[27/180] Gradient Norm: 4706.0908203125\n",
      "[27/180] lr: 3.6444890691097577e-06\n",
      "[28/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024c400c46b944318fbd34ae060c71b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3d9b1321834b219026a243a57eb9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/180] Train Loss: 0.5116999745368958\n",
      "[28/180] Valid Loss: 0.5547999739646912\n",
      "[28/180] Pearson Score: 0.8408\n",
      "[28/180] Gradient Norm: 2299.37841796875\n",
      "[28/180] lr: 7.392323026181453e-07\n",
      "[29/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0a3138d5884e0eb8bb1060c2925040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26c4bceecfd4e49b8dd2a392d634791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/180] Train Loss: 0.5105999708175659\n",
      "[29/180] Valid Loss: 0.5533999800682068\n",
      "[29/180] Pearson Score: 0.8417\n",
      "[29/180] Gradient Norm: 3829.83349609375\n",
      "[29/180] lr: 4.995624660278707e-05\n",
      "[Update] Valid Score : (0.8414 => 0.8417) Save Parameter\n",
      "Best Score: 0.8417452063837799\n",
      "[30/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a88917f5954cd980d2a5ca1f17dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8756d6c37d0c4ac78bac2fd7f17db43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/180] Train Loss: 0.5120000243186951\n",
      "[30/180] Valid Loss: 0.5551999807357788\n",
      "[30/180] Pearson Score: 0.8417\n",
      "[30/180] Gradient Norm: 3008.890380859375\n",
      "[30/180] lr: 4.8378600357061824e-05\n",
      "[31/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0716beb72f447aa8c1daea52e19f20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79445438e10a41059d4ce724c71d0fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/180] Train Loss: 0.5110999941825867\n",
      "[31/180] Valid Loss: 0.5514000058174133\n",
      "[31/180] Pearson Score: 0.8404\n",
      "[31/180] Gradient Norm: 1851.47314453125\n",
      "[31/180] lr: 4.4671527947390544e-05\n",
      "[32/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557d1bd203df45beb4d7d084b6561243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6fb71ba2924606a40e7923f83a3f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/180] Train Loss: 0.5120000243186951\n",
      "[32/180] Valid Loss: 0.5611000061035156\n",
      "[32/180] Pearson Score: 0.8379\n",
      "[32/180] Gradient Norm: 4354.42138671875\n",
      "[32/180] lr: 3.917268589983689e-05\n",
      "[33/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f99310216ba48cbb60d5153400c80b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a8ccbd63d7461eb8b52d45d0e7bb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/180] Train Loss: 0.5091000199317932\n",
      "[33/180] Valid Loss: 0.5569000244140625\n",
      "[33/180] Pearson Score: 0.8416\n",
      "[33/180] Gradient Norm: 8824.7109375\n",
      "[33/180] lr: 3.23829330318078e-05\n",
      "[34/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5f2279e11c4a878444f96cd5eac8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7029249c050467e962f0ff92e573a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/180] Train Loss: 0.5092999935150146\n",
      "[34/180] Valid Loss: 0.5547000169754028\n",
      "[34/180] Pearson Score: 0.8418\n",
      "[34/180] Gradient Norm: 2301.409912109375\n",
      "[34/180] lr: 2.4920710019096003e-05\n",
      "[Update] Valid Score : (0.8417 => 0.8418) Save Parameter\n",
      "Best Score: 0.841761358055445\n",
      "[35/180] Train & Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e983b1490e4c3e80dd29a33eedc957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is Test Code for Appending Cross-Validaton Strategy (fold to epoch)\n",
    "cfg_list = [CFG]\n",
    "for cfg in cfg_list:\n",
    "    #init wandb\n",
    "    wandb.init(project=\"[Append Ver 2]UPPPM Token Classification\", \n",
    "               name='[Append Version 2.2]' + 'Fold 0' + cfg.model_name,\n",
    "               config=class2dict(cfg),\n",
    "               group=cfg.model_name,\n",
    "               job_type=\"train\",\n",
    "               entity = \"qcqced\")\n",
    "    wandb_config = wandb.config\n",
    "    print(f'========================= Retriever Model :{cfg.model_name} =========================')\n",
    "    fold_list, swa_score_max = [i for i in range(cfg.n_folds)], -np.inf\n",
    "    \n",
    "    for fold in tqdm(fold_list):\n",
    "        print(f'============== {fold+1}th Fold Train & Validation ==============')\n",
    "        val_score_max = -np.inf\n",
    "        fold_train_loss_list, fold_valid_loss_list, fold_score_list  = [], [], []\n",
    "        fold_swa_loss, fold_swa_score = [], []\n",
    "        \n",
    "        train_input = TrainInput(cfg, train_df) # init object\n",
    "        model, swa_model, criterion, optimizer, save_parameter = train_input.model_setting()\n",
    "        loader_train, loader_valid, train, valid, valid_labels = train_input.make_batch(fold)\n",
    "        \n",
    "        # Scheduler Setting\n",
    "        if cfg.swa:\n",
    "            swa_start = cfg.swa_start\n",
    "            swa_scheduler = SWALR(\n",
    "                optimizer,\n",
    "                swa_lr = cfg.swa_lr, # Later Append\n",
    "                anneal_epochs=cfg.anneal_epochs, \n",
    "                anneal_strategy=cfg.anneal_strategy\n",
    "            )\n",
    "            \n",
    "        if cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps) * cfg.warmup_ratio,\n",
    "                num_training_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps),\n",
    "                num_cycles = cfg.num_cycles\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine_annealing':\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps) * cfg.warmup_ratio,\n",
    "                num_training_steps=int(len(train)/cfg.batch_size * cfg.epochs/cfg.n_gradient_accumulation_steps),\n",
    "                num_cycles = 8 \n",
    "            )\n",
    "        else:\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs) * cfg.warmup_ratio,\n",
    "                num_training_steps=int(len(train) /cfg.batch_size * cfg.epochs),\n",
    "                num_cycles = cfg.num_cycles\n",
    "            )  \n",
    "\n",
    "        for epoch in range(cfg.epochs):\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Train & Validation')\n",
    "            if cfg.swa:\n",
    "                train_loss, valid_loss, score, grad_norm, lr = train_fn(\n",
    "                    cfg,\n",
    "                    loader_train,\n",
    "                    loader_valid,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    valid,\n",
    "                    valid_labels,\n",
    "                    int(epoch),\n",
    "                    swa_model=swa_model,\n",
    "                    swa_start=swa_start,\n",
    "                    swa_scheduler=swa_scheduler,\n",
    "            )        \n",
    "            else:\n",
    "                train_loss, valid_loss, score = train_fn(\n",
    "                    cfg,\n",
    "                    loader_train,\n",
    "                    loader_valid,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    valid,\n",
    "                    valid_labels,\n",
    "                    int(epoch),\n",
    "            )\n",
    "            \n",
    "            train_loss = train_loss.detach().cpu().numpy()\n",
    "            valid_loss = valid_loss.detach().cpu().numpy()\n",
    "            grad_norm = grad_norm.detach().cpu().numpy()\n",
    "            \n",
    "            fold_train_loss_list.append(train_loss)\n",
    "            fold_valid_loss_list.append(valid_loss)\n",
    "            fold_score_list.append(score)\n",
    "           \n",
    "            wandb.log({\n",
    "                '<epoch> Train Loss': train_loss,\n",
    "                '<epoch> Valid Loss': valid_loss,\n",
    "                '<epoch> Pearson_Score': score,\n",
    "                '<epoch> Gradient Norm': grad_norm,\n",
    "                '<epoch> lr': lr\n",
    "            })\n",
    "            \n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Train Loss: {np.round(train_loss, 4)}') \n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Valid Loss: {np.round(valid_loss, 4)}')\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Pearson Score: {np.round(score, 4)}')\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] Gradient Norm: {np.round(grad_norm, 4)}')\n",
    "            print(f'[{epoch+1}/{cfg.epochs}] lr: {lr}')\n",
    "            \n",
    "        \n",
    "            if val_score_max <= score:\n",
    "                print(f'[Update] Valid Score : ({val_score_max:.4f} => {score:.4f}) Save Parameter')\n",
    "                print(f'Best Score: {score}')\n",
    "                torch.save(model.state_dict(),\n",
    "                           f'Ver2-3_Token_Classification_Fold{fold}_DeBERTa_V3_Large.pth')\n",
    "                val_score_max = score\n",
    "            \n",
    "        del train_loss, valid_loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f'================= {fold+1}th Train & Validation =================')            \n",
    "        fold_train_loss = np.mean(fold_train_loss_list)\n",
    "        fold_valid_loss = np.mean(fold_valid_loss_list)\n",
    "        fold_score = np.mean(fold_score_list)\n",
    "        wandb.log({f'<Fold{fold+1}> Train Loss': fold_train_loss,\n",
    "                   f'<Fold{fold+1}> Valid Loss': fold_valid_loss,\n",
    "                   f'<Fold{fold+1}> Pearson_Score': fold_score,})\n",
    "        print(f'Fold[{fold+1}/{fold_list[-1]+1}] Train Loss: {np.round(fold_train_loss, 4)}')\n",
    "        print(f'Fold[{fold+1}/{fold_list[-1]+1}] Valid Loss: {np.round(fold_valid_loss, 4)}')\n",
    "        print(f'Fold[{fold+1}/{fold_list[-1]+1}] Pearson Score: {np.round(fold_score, 4)}')\n",
    "        \n",
    "        if cfg.swa:\n",
    "            update_bn(loader_train, swa_model) # Stochastic Weight Averaging\n",
    "            fold_swa_loss, fold_swa_score = swa_valid(\n",
    "                cfg,\n",
    "                loader_valid,\n",
    "                swa_model,\n",
    "                criterion,\n",
    "                valid_labels,\n",
    "            )\n",
    "            fold_swa_loss = fold_swa_loss.detach().cpu().numpy()\n",
    "            fold_swa_loss = np.mean(fold_swa_loss)\n",
    "            fold_swa_score = np.mean(fold_swa_score)\n",
    "            \n",
    "            wandb.log({\n",
    "                f'<Fold{fold+1}> SWA Valid Loss': fold_swa_loss,\n",
    "                f'<Fold{fold+1}> SWA Pearson_Score': fold_swa_score,\n",
    "            })\n",
    "            \n",
    "            print(f'Fold[{fold+1}/{fold_list[-1]+1}] SWA Loss: {np.round(fold_swa_loss, 4)}')\n",
    "            print(f'Fold[{fold+1}/{fold_list[-1]+1}] SWA Score: {np.round(fold_swa_score, 4)}') \n",
    "        \n",
    "        if val_score_max <= fold_swa_score:\n",
    "            print(f'[Update] Valid Score : ({val_score_max:.4f} => {fold_swa_score:.4f}) Save Parameter')\n",
    "            print(f'Best Score: {fold_swa_score}')\n",
    "            torch.save(model.state_dict(),\n",
    "                       f'SWA_Ver2-3_Token_Classification_Fold{fold}_DeBERTa_V3_Large.pth')\n",
    "            val_score_max = fold_score\n",
    "            \n",
    "        del fold_swa_loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Append Ver 1]\n",
    "1) Add Gradient Accumulation & Clipping Norm\n",
    "- ëŠë¦¬ì§€ë§Œ í™•ì‹¤ížˆ ì•ˆì •ì ìœ¼ë¡œ ìŠ¤ì½”ì–´ ìƒìŠ¹, ë‹¤ìŒì— Clipping Norm ì„¤ì • ë„ê³  epochë³„ Gradient ì´ëŸ‰ì„ êµ¬í•´ì„œ max_norm ê°’ì„ accumulation step ìˆ«ìžì— ë§žê²Œ ìˆ˜ì •í•˜ìž  \n",
    "- í™•ì‹¤ížˆ max_grad_normì„ ì˜¬ë¦¬ë‹ˆê¹Œ ì²˜ìŒë¶€í„° ì–´ì§€ê°„í•œ ì„±ëŠ¥ì´ ë‚˜ì˜¤ëŠ” ë“¯  \n",
    "- Running Time ìƒê° ì•ˆí•˜ê³  Batch & Epoch ê´€ì ì—ì„œë§Œ ë³´ë©´ í›¨ì”¬ ì„±ëŠ¥ì´ ì¢‹ì•„ ì§„ ê±°ê¸´ í•¨  \n",
    "- learning rateë„ accumulation ìˆ«ìžì— ë§žê²Œ ë‚˜ëˆ ì¤˜ì•¼ í•˜ëŠ”ê±° ì•„ë‹Œê°€?? í•œë²ˆ í•´ë³´ìž\n",
    "\n",
    "2) Add Stochastic Weight Averaging  \n",
    "- OOM ë°œìƒ ì•ˆí•˜ê³  ìž˜ ëŒì•„ê°„ë‹¤ \n",
    "- VRAMì´ ì§„ì§œ 10% ì •ë„ ìƒìŠ¹í•¨ \n",
    "- ì´ì œ ë¬¸ì œëŠ”..... AWP  \n",
    "- epoch 8 ì„¤ì •í•˜ê³  í•œë²ˆ ì¼ë°˜ Validationì´ëž‘ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ í™•ì¸í•´ë³´ìž  \n",
    "  ì¼ë°˜ ëª¨ë¸ í´ë“œ í‰ê· : 0.51  \n",
    "  SWA ëª¨ë¸: 0.60  \n",
    "\n",
    "3) Adversarial Weight Pertubation  \n",
    "- token max_len = 400 => ì•„ìŠ¬ì•„ìŠ¬í•˜ê²Œ ëŒì•„ê°\n",
    "- ê·¼ë° í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ëŠë ¤ì„œ ëª»ì¨ë¨¹ê² ìŒ  \n",
    "- ì´ê±° ê°™ì´ ì“¸ê±°ë©´ Gradient Normì„ ì˜¬ë¦¬ë˜ í•™ìŠµë¥ ì„ ì˜¬ë¦¬ë˜ í•´ì•¼ê² ìŒ  \n",
    "\n",
    "4) Cosine Scheduler  \n",
    "- num_cycle: 8  \n",
    "  [ì´ˆê¸°]  \n",
    "  epoch=180, num_accumulation = 4, num_cycle =2\n",
    "  ì‹¤ì œ ìŠ¤ì¼€ì¤„ëŸ¬ stepì€ 45íšŒ ë°œìƒ, ì‹¤í—˜ ê²°ê³¼ ì—í­ 60íšŒ(ì‹¤ ì—í­ 15íšŒ) ë¶€í„° ìŠ¤ì½”ì–´ê°€ ìˆ˜ë ´í•˜ëŠ” ê²½í–¥ì„±ì„ ë³´ìž„  \n",
    "  num_cycle = 2ë¡œ ì„¤ì •í–ˆê¸° ë•Œë¬¸ì—, ì‚¬ì‹¤ìƒ í•œ ì£¼ê¸°ì˜ ì ˆë°˜ì¯¤ë¶€í„° ìˆ˜ë ´ì´ ë°œìƒí•¨  \n",
    "  ë‹¨ìˆœí•˜ê²Œ ìƒê°í•´ë³´ë©´ num_cycle * 2 * = 16ìœ¼ë¡œ ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ì˜ num_cycle ì„¤ì •í•˜ëŠ” ê²ƒì´ ë§žì•„ë³´ì´ì§€ë§Œ,  \n",
    "  í•™ìŠµë¥ ì˜ ê°ì†Œí•˜ëŠ” ì¶”ì„¸ë¥¼ ê³ ë ¤í–ˆì„ë•Œ 8ì •ë„ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¼ë‹¨ ì¢‹ì•„ ë³´ì¸ë‹¤.\n",
    "  ì´ëŸ° ê³„ì‚° ë° ê°€ì •í•˜ëŠ” ê²ƒì´ ê·€ì°®ì„ ë¿ë”ëŸ¬ ì •í™•í•œ í•™ìŠµ ê²°ê³¼ë¥¼ ë³´ìž¥í•˜ëŠ” ê²ƒë„ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì–¼ë¥¸ íŒŒë¼ë¯¸í„° íŠœë‹ íˆ´ì„ ì ìš©í•˜ìž  \n",
    "  \n",
    "[Append Ver 2]  \n",
    "1) num_accumulation = 1\n",
    "- ì´ê±° ì ìš©í•˜ëŠ” ê²ƒì˜ ì´ì ì´ ë”±ížˆ ì—†ëŠ”ë“¯  \n",
    "- AWP = False => max_len = 512\n",
    "2) Clipping Grad Norm = 1000  \n",
    "\n",
    "3) Cosine Annealing Scheduler  \n",
    "- num_cycle: 8, 16  \n",
    "\n",
    "4) Re-Init Top Encoder Block  \n",
    "- num_reinit: 5  \n",
    "\n",
    "5) Cross Validation  \n",
    "- num_fold: 4  \n",
    "  ì‹¤ì œ ëŒ€íšŒ LBëŠ” 25%, PBëŠ” 75%ë¼ì„œ í•´ë‹¹ ë¹„ìœ¨ì— ë§žê²Œ í´ë“œë¥¼ êµ¬ì„±í•˜ìž...CVì˜ ì¤‘ìš”ì„±ì€ ì§„ì§œ 100000ì–µë²ˆ ë°˜ë³µí•´ë„ ì§€ë‚˜ì¹˜ì§€ ì•Šì€ ê²ƒ ê°™ë‹¤  \n",
    "  \n",
    "6) Add LR Tracker  \n",
    "\n",
    "7) Append LR  \n",
    "- options: 1e-5, 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, model, swa_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
